---
title: Evaluating a simplified forecast model in comparison to the ECDC forecasting hub ensemble
preprint: false
author: 
  - name: Sam Abbott
    affiliation: 1
    corresponding: true
    email: sam.abbott@lshtm.ac.uk
affiliation:
  - code: 1
    address: Cold Spring Harbor Laboratory, One Bungtown Road Cold Spring Harbor, NY 11724
abstract: > 
  We made the best model. It is both simple and amazing.
bibliography: library.bib
output:
  rticles::peerj_article: default
  bookdown::pdf_book:
    base_format: rticles::peerj_article
    extra_dependencies: ["float"]
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = TRUE,
  echo = FALSE,
  message = FALSE,
  cache = TRUE,
  messages = FALSE,
  warnings = FALSE
)
```

# Introduction {-}

- Why
  - ECDC and CDC forecasting hubs
  - Ensemble shown to outperform individual models in many cases
  - Thought of as being more desirable due to reliability etc vs multiple models
  - Many models are complex and require large amounts of compute time.
  - As the ensemble is made up of these models it means it is extremely resource demandning to construct requiring significant compute as well as multiple teams.
  - As the ensemble is constructed from multiple models it functions as effectively a black box. This means it may be hard for policy-makers to understand its underyling assumptions
  - Here we seek to refine forecasting ideas from our previous work alongside our experiances forecasting COVID-19 as contributors to the ECDC and CDC forecasting hubs to develop a simplified forecasting model that has characteristics fo the ensemble forecast whilst maintianing a interpretability and low resource demands.

- Extension of why
   - Brief dicussion of implications of modelling NPIs/future changes in behaviour
   - Brief discussion of reporting patterns in data. Daily/Weekly.
   - Brief discussion of EpiNow2 and related models
   - Brief dicussion of statistical modelling approaches (use MUNI-ARIMA as an example).
   - Brief discussion of scoring of hub models
   - Summary of observed performance of the hub ensembles
    - Auto-correlated
    - Typically relatively trend following though shows some ability to forecast future changes. 

- What
  - Simplified auto-regressive growth rate model assuming a non-stationary trend
  -  Explicit modelling of the assumption that rapid growth/decay of cases will typically result in future growth tending towards zero. This seeks to capture potential implementation and relaxation of NPIs as well as individuals attitudes to risk (assuming this is based on current growth rates and not case incidence).
  - Uses weekly vs daily data to side step issues of reporting periodicity and to replicate the observed auto-corelation of the hub ensembles.
  - Simple extension of an already implemented model and resuse of its modular components.
  - Implemented with a focus on reproducibility, simplicity, and low resource use. 
  - The model runs using GitHub actions in a framework supplied by the ECDC forecast hub. We provide our own tools so that others can run the model using GitHub actions democratising our approach.

- Aim: In this work we define a simplified forecasting model designed to replicate some of the observed characteristics of successful infectious disease ensemble models without sacrificing interpretability and with low resource requirements. We evaluate the performance of this model against the ECDC forecasting hub ensemble in real-tim, and discuss implications of our findings along with areas for future research.

# Methods {-}

## Data {-}

```{r packages}
library(data.table)
library(scoringutils)
library(ggplot2)
library(scales)
library(here)
```

```{r load-functions}
source(here("R", "utils.R"))
```

```{r data}
# Load JHU frozen truth data. See data-raw/get-truth.R for munging
truth <- fread(here("data", "truth.csv"))

# Load population data
population <- fread(here("data", "population.csv"))

# Load ECDC forecast metadata. See data-raw/get-hub-metadata.R for munging
#metadata <- fread(here("data", "forecast-metadata.csv"))

# Load ECDC forecasts. See data-raw/get-hub-forecasts.R for munging
# Merge with truth and rescale to incidence rate per 100,000
forecasts <- fread(here("data", "forecasts.csv")) |>
  merge_forecasts_with_truth(truth) |>
  rescale_to_incidence_rate(population, scale = 1e4) |>
  rename_models()

# Load forecast scores
scores <- fread(here("data", "scores.csv")) |>
  rename_models()
```

- Data from JHU as used as truth data by the ECDC forecasting hub.
  - When accessed
  - Aggregated to epiweek as in the ECDC forecasting GitHub
  - Countries included
  - Dates of data included
- Forecasts from the ECDC forecast hub
  - Forecasts dowloaded for the model proposed here and the ECDC ensemble
  - Forecast format
  - Forecases from when and horizon
  - Ensemble construction and reference
  
## Model {-}

### Motivation

The same underlying structure with expected notifications being assumed to be a product of past expected notifications, and an exponentiated time-varying term.
This term is then modelled in both instances as a differenced autoregressive process with a single lag term.
By default it is assumed to vary inline with the timestep of the data but this can be altered to speed up computation or to improve out of sample performance.
Observed cases are estimated from expected cases by assuming a negative binomial observation model.
A detailed model definition is given below.

### Definition

We model the expectation ($\lambda_t$) of reported cases ($C_t$) as an order 1 autoregressive (AR(P)) process by epidemiological week ($t$).
The model is initialised by assuming that the initial reported cases are representative with a small amount of error (2.5%).
We assume a negative binomial observation model with overdispersion $\phi$ for reported cases ($C_t$).

\begin{align*}
  \lambda_0 &\sim \text{LogNormal}\left(\log C_0 , 0.025 \times \log C_0 \right)\\
  \lambda_t &= C_{t-1} e^{r_t},\ t > 0  \\
  C_{t} \mid \lambda_{t} &\sim \text{NB}\left(\lambda_t, \phi\right)
\end{align*}

Where $r_t$ can be interpreted as the growth rate. $r_t$ is then modelled as a piecewise constant differenced AR(1) process modified such that the dependence of $r_{t-1}$ is multiplied by a decay factor ($\xi_{+,-}$) that varies dynamically according to the sign of $r_{t-1}$.
The assumptions of this modelling approach are that the growth rate is non-stationary with a trend that is independent of the current growth rate (the differenced AR(1) process), the additional decay factor encodes the belief that larger absolute growth rates will tend more quickly towards no growth and that this process may work differently for positive or negative growth rates.
This process can be defined as follows, 

\begin{align*}
  r_0 &\sim \text{Normal}\left(0, 0.25 \right) \\
  r_t &= \left(\mathcal{H}(r_{t-1} > 0) \xi_{+} + \mathcal{H}(r_{t-1} \le 0) \xi_{-}\right)r_{t-1} + \epsilon_t  \\
  \epsilon_t &= \mathcal{H}(t > 0) \beta \epsilon_{t-1} + \eta_t
\end{align*}

Where $\mathcal{H}$ is the Heaviside step function and is defined such that it attains the value of 1 if the argument is true and the value of 0 otherwise.
The following priors are used,

\begin{align*}
  \xi_{+} &\sim \text{Beta}\left(3, 1 \right) \\
  \xi_{-} &\sim \text{Beta}\left(3, 1 \right) \\
  \beta &\sim \text{Normal}\left(0, 0.25 \right) \\
  \eta_t &\sim \text{Half-Normal}\left(0, \sigma \right) \\
  \sigma &\sim \text{Normal}\left(0, 0.2 \right) \\
  \frac{1}{\sqrt{\phi}} &\sim \text{Half-Normal}(0, 1) 
\end{align*}

Where $\sigma$, and $\frac{1}{\sqrt{phi}}$ are truncated to be greater than 0 and $\beta$ is truncated to be between -1 and 1.
The Beta priors for $\xi{+,-}$ have been chosen to be weakly informative that the reduction towards 0 growth is relatively slow.
Similarly the prior for $\beta$ has been chosen to be weakly informative that there is weak auto-correlation in differenced growth rates.
$\sigma$ has also been made weakly informative under the assumption that the potential change in growth rates in a single timestep should be relatively small.

### Summary statistics

As well as posterior predictions and forecasts for notifcations the model also returns several epidemiological summary statistics which may be useful for drawing inferences about underlying transmission dynamics.
These are the log scale growth rate ($g^{o, \delta}_t$), and the instantaneous effective reproduction number ($R^{o, \delta}_t$).
These are calculated as follows:

\begin{align*}
  g^{o, \delta}_t &= T_s r^{o, \delta}_t \\
  R^{o, \delta}_t &= e^{\left(T_s r^{o, \delta}_t\right)} \\
\end{align*}

$T_s$ is a user set scaling parameter that defines the timespan over which the summary metrics apply dependent on the time step of the data.
It can be set using the `scale_r` and defaults to 1 which returns summary statistics scaled to the timestep of the data.
Depending on the setup of the model used these summary measures will be more or less related to their epidemiological definitions.
In particular, adding a weighting to past expected cases that is more complex than a simple lag may cause interpretation issues.

## Forecast evaluation {-}

- Visualise forecasts across time horizons in examplar countries.
- Weighted interval score.
- Calibration and coverage.
- Relative interval score.
- Relative interval score overall, distribution of scores, by horizon both overall and distribution, by location overall and distribution.
- Relative interval score by interval range by horizon
- Calibration at 50 and 90 credible intervals
- Coverage across intervals.

```{r coverage}
ranges <- c(30, 60, 90)

coverage <- scores |>
  copy() |>
  DT(, horizon := as.character(horizon)) |>
  DT(, coverage_deviation := NULL) |>
  add_coverage(ranges = ranges, by = c("model", "horizon")) |>
  summarise_scores(fun = signif, digits = 2, by = c("model", "horizon")) |>
  melt(
    measure.vars = patterns("coverage_"), variable.name = "range",
    value.name = "coverage"
  )|>
  DT(, range := gsub("coverage_", "", range)) |>
  DT(, range := paste0(range, "% interval"))
```

```{r relative-interval-score}
cols <- c("location", "target_end_date", "horizon", "range", "model")
relative_interval_score <- scores |>
  copy() |>
  setorderv(cols) |>
  DT(, c(..cols, "interval_score")) |>
  unique() |>
  DT(,
    relative_interval_score := interval_score / shift(interval_score),
    by = c("location", "target_end_date", "horizon", "range")
  ) |>
  DT(model %in% "Surrogate")
```

## Implementation {-}

The model is implemented in `stan` [@stan] and `R` (`4.2.0`) [@R] as an extension of the baseline model from the `forecast.vocs` R package (`0.0.9.7000`) [@forecast.vocs].
The `cmdstanr` R package (`0.5.2`) [@cmdstanr] is used for model fitting with 2 MCMC chains each having 1000 warm-up and 1000 sampling steps each [@cmdstanr].
`cmstanr` surfaces several settings which trade-off between sampling speed and the robustness of approach
Here we take a conservative approach, as model fit is not manually inspected during real-time usage and due to the expected complexity of the posterior [@betancourt_2017], and set the adapt delta setting to 0.99, and the maximum treedepth setting to 15. 
For real-time usage convergance was not assessed but during model development the Rhat diagnostic was used alongside feedback from `cmdstanr` about the number of divergant transitions and exceedance of the maximum tree depth and posterior predictions visually compared to observed data [@cmdstanr].

To download and manipulate forecasts from the ECDC forecasting hub [@EuroHub] we use  the `data.table` (`1.14.2`) [@data.table] and `gh` (`1.3.0`) [@gh] R packages.
We make use of further functionality from the `forecast.vocs` R package [@forecast.vocs] to prepare data for forecasting, to visualise forecasts and summary measures, and to summarise forecasts.
Forecast evaluation is implemented using the `scoringutils` R package (1.0.0) [@scoringutils].

To ensure reproducibility of this analysis dependencies are managed using the `renv` R package (`0.14.0`) [@renv] and a Dockerfile file along with built Docker image [@Boettiger:2015dw] (via GitHub Actions) is provided.
Weekly forecasts also made use of `renv` and Docker to ensure reproducibility with GitHub actions used to produce the forecasts ensuring we meet our limited compute goal and that our implementation is independent from available compute resources democratising  access to our approach.
The code for this analysis can be found here: https://github.com/epiforecasts/simplified-forecaster-evaluation
The code for the forecating model defined above along with the infrastructure required to forecast using GitHub Actions can be found here: https://github.com/seabbs/ecdc-weekly-growth-forecasts

# Results {-}

## Data summary {-}

- Date range of forecasts
- Forecasts per country
- Total number of forecast models and average per location and per forecast date
- Summarises general trends in incidence in the data

## Visualisation of forecasts by horizon {-}

```{r vis-forecasts, fig.cap = "Forecasts of notified test positive cases (per 10,000 population) by epidemiological week in the Germany, Greece, Italy, Poland, Slovakia, and the United Kingdom,  by forecast horizon (one to four weeks). Incidence rates are shown on a log scale. 30\\%, 60\\%, and 90\\% credible intervals are shown. The black line and points are the notified cases as of the date of data extraction rather than those available at the time.", out.width = "95%", fig.height = 12, fig.width = 9}
locs <- c("United Kingdom", "Germany", "Slovakia", "Italy", "Poland", "Greece")

forecasts[location_name %in% locs] |>
  DT(, date := target_end_date) |>
  plot_predictions(
    by = c("horizon", "location_name"), range = c(30, 60, 90)
  ) +
  facet_grid(
    location_name ~ horizon, scales = "free_y"
  ) +
  aes(fill = model, col = model) +
  scale_y_log10(labels = comma, oob = squish, limits = c(NA, NA)) +
  scale_fill_brewer(palette = "Dark2") +
  scale_color_brewer(palette = "Dark2") +
  labs(x = "Date", fill = "Model", col = "Model", ramp = "Range",
        y = "Notified test positive cases per 10,000 population")
```

## Forecast evaluation {-}

- Figure 2:
  Sub-figure 1:
    - Density/histogram of overall relative scores + mean line
  Sub-figure 2:
    - Density/histogram of relative scores + mean line by horizon
  Sub-figure 3:
     - Density/histogram of relative scores + mean line by location

## Forecast calibration {-}

- Figure 3:
  Sub-figure 1:
    - Calibraton of forecasts by horizon and model
  Sub-figure 2:
    - Coverage of forecasts by quantile
  Sub-figure 3:
    - Relative interval score by quantile by horizon.


```{r plot-coverage, fig.height = 9, fig.width = 9, out.width = "95%"}
hlines <- data.table(
  range = paste0(ranges, "% interval"), nominal = ranges / 100
)

plot_cov_levels <- ggplot(coverage) +
  aes(
    x = horizon, y = coverage, colour = model, group = interaction(range, model)
  ) +
  geom_line() +
  geom_point() +
  geom_hline(
    data = hlines, aes(yintercept = nominal, x = NULL), linetype = "dashed"
  ) +
  scale_colour_brewer(palette = "Dark2") +
  scale_y_continuous(labels = percent) +
  facet_wrap(vars(range), ncol = 1, scales = "free_y") +
  theme_scoringutils() +
  labs(
     y = "Proportion of data within forecast interval",
     x = "Forecast horizon (weeks)",
     col = "Model"
  )

plot_cov_quantile <- scores |>
  summarise_scores(by = c("model", "quantile", "horizon")) |>
  plot_quantile_coverage(scores) +
  aes(col = model) +
  scale_x_continuous(labels = percent) +
  scale_y_continuous(labels = percent) +
  scale_colour_brewer(palette = "Dark2") +
  guides(col = guide_none()) +
  facet_wrap(vars(horizon)) +
  labs(col = "Model", y = "Percent of observations below quantile")

plot_interval_score <- relative_interval_score |>
  copy() |>
  DT(,
    .(relative_interval_score = median(relative_interval_score)),
    by = c("horizon", "range")
  ) |>
  ggplot() +
  aes(x = range / 100, y = relative_interval_score, col = as.factor(horizon)) +
  geom_hline(yintercept = 1, linetype = 2) +
  geom_point(size = 1.2) +
  geom_line(alpha = 0.8) +
  scale_x_continuous(labels = percent) +
  scale_y_continuous(trans = "log", breaks = seq(1, 4, by = 0.2)) +
  scale_colour_brewer(palette = "Accent") +
  theme_scoringutils() +
  labs(
    x = "Quantile", y = "Median relative interval score",
    col = "Forecast horizon (weeks)"
  )

(((plot_cov_levels /
  plot_interval_score)  +
  plot_layout(heights = c(3, 1), guides = "collect")
) |
  plot_cov_quantile) +
  plot_layout(guides = "collect", widths = c(1, 2)) +
  plot_annotation(tag_levels = "a") &
  theme(
    legend.position = "bottom", legend.box="vertical", legend.margin = margin()
  )
```

# Discussion {-}

## Summary {-}

## Stengths and weaknesses {-}

## Literature context {-}

  - Emulators in weather forecasting and elsewhere
  - ECDC and CDC ensemble papers
  - Other forecasting models contributed to the ensembles
  - Baseline models

## Further work {-}

  - Other similar model formulations. More lag terms. Reduced run-times using approximate Bayesian approaches.
  
## Conclusions {-}

# Acknowledgments {-}

# References
