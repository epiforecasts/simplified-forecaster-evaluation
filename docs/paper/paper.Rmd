---
title: Evaluating a simplified forecast model in comparison to the ECDC forecasting hub ensemble
preprint: false
author: 
  - name: Sam Abbott
    affiliation: 1
    corresponding: true
    email: sam.abbott@lshtm.ac.uk
  - name:  Johannes Bracher
    affiliation: 2
  - name: Sebastian Funk
    affiliation: 1
affiliation:
  - code: 1
    address: London School of Hygiene and Tropical Medicine, London, UK
  - code: 2
    address: Chair of Statistics and Econometrics, Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany
abstract: > 
  We made the best model. It is both simple and amazing.
bibliography: library.bib
output:
  rticles::peerj_article: default
  bookdown::pdf_book:
    base_format: rticles::peerj_article
    extra_dependencies: ["float"]
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = TRUE,
  echo = FALSE,
  message = FALSE,
  cache = TRUE,
  messages = FALSE,
  warnings = FALSE
)
```

# Introduction {-}

Multi-model and  multi-team ensembles have become increasingly popular as an approach to increase the robustness and performance of infectious disease forecasts over the last decade with significant success [@REF].
These approaches have drawn heavily on the experiance of other domains, for example climate modelling [@REF], where ensembles of both multiple models and from multiple teams have a long history of providing superior forecasts.
However, ensemble forecasts have range of potential downsides with the most obvious being their considerable cost to produce as they typically require contributions from multiple independent teams, the development of several models, and a centralised group to run the ensembling project, and that forecasts from these ensembles can be difficult to link to the underling model assumption.
In climate forecasting emulation approaches have been used to circumvent some of the  resource requirment issue by training a simplified model, usually a non-parametric statistical model, to replicate the behaviour of either the entire model or sub-components [@REF].
However, within epidemiological modelling we often want to use our models for applications other than forecasting, have fewer insights beyond forecast performance on which to base our mechanistic assumptions. 
This means that non-parametric emulation is not an ideal solution. 
In this work we propose and evaluate a surrogate forecast model approach which seeks to emulate the observed behaviour of recent COVID-19 forecast ensembles using a minimal set of easily communicated assumptions, limited computational resources, and an easily generalised implementation. 

The trend towards large-scale multi-team ensemble forecasting in infectious diseases has accelerated during the COVID-19 pandemic, partially due to an increased need from stakeholders for reliable forecast models but also due to the percieved low quality of many forecasts otherwise being produced, with the establishment of the CDC forecasting hub [@REF] in early 2020, the Germany/Poland forecast hub [@REF] in late 2020, and ECDC forecast hub [@ECDC] in 2021. All of these collaborations, which the authors (particularly SF and JB have been involved with managing),  ensemble contributions from multiple independent teams using a similar ensembling approach (most recently an unweighted median ensemble [@REF]) and have shown that their ensemble forecasts outperform most individually contributed forecasts and provide forecasts that are more robust to outliers [@REF].
However, they have struggled to provide feedback to those contributing forecasts that would allow them to improve their forecast approaches, have found it difficult to improve the quality of their ensemble forecasts [@REF], and have struggled to maintain incentives for forecasters to continue to contribute which may impact the long-term quality of the  ensembles they produce. 

These forecast collaborations, and other similar infectious disease forecasting efforts, have focussed on forecasting reported cases and deaths by epidemiological week (i.e Saturday to Saturday) over a 1 to 4 week time horizon. Generally ground truth data is available daily, and is updated in retrospect as new data becomes available. Typically, the ensemble forecast produced each week is made up of at least 5 independent forecasts using a range of approaches and is produced using an unweighted median ensemble [@REF].
The majority of submitted forecasts make use of the available daily data, along with potentially other leading predictors to produce daily forecasts that are then aggregated to weekly forecasts.
Many use a range of outlier processing techniques to reduce reporting artefacts which can bias forecasts.
In general, 4 main classes of forecast models are used, statistical forecasting models such as ARIMA models [@REF], mechanistic forecasting models based on the compartmental modelling framework and it's generalisations [@REF], semi-mechanistic approaches that blend both of these approaches [@REF], and human insight based forecast models that may also include elements of other methods [@REF].
Real-time evaluation has shown that each of these classes of models may perform well though in general semi-mechanistic models, and models that include human insights have outperformed other approaches during certain periods of time [@REF].
We have contributed a range of forecasts to these forecast collobarations generally focussed on semi-mechanistic statistical methods [@REF], and human insight based forecasts [@REF] with our forecasts performing relatively well compared to the majority of those submitted but rarely outperforming significantly.
The model based forecasts we have contributed have focussed on trying to carefully model the underlying infectious disease dynamics from infection through to symptom onset, and test positivity using non-exponential delay distributions whilst also attempting to model the complexity of daily, within week, reporting periodicity [@REF].
Based on our observations our forecasts have generally captured changes in trend better than most submitted models that do not use human insight based on known policy changes but have not been robust to reporting issues such as large outliers in reporting and changes to reporting patterns. 
Our previous methodology also requires signinficant computational resources running for an hour on a 16 core machine when producing forecasts from the ECDC. 
In our model based forecasts we did not attempt to capture potential future interventions or known interventions not currently observed in the epidemiological data whereas in our human insight models these were implicitly included.
We found that our human insight based forecasts significantly outperformed our model based forecasts and hypothesised that this may have been driven by including additional information not observed in the epidemiological data [@REF]. 
Unlike our epidemiologically motivated forecast submissions the hub ensemble forecasts were typically robust to daily reporting artefacts.
They also demonstrated some ability to forecast future changes in trend that were not present in the observed data similarly to our human insight forecasts indicating the likely inclusion of either human insight or assumptions about future interventions. 
In general, the ensemble forecasts were more auto-correlated and less reactive to changes from stable or reducing case incidence to increasing incidence.
On the other hand this also meant that the ensemble was less likely to adopt short-term changes in incidence and hence produced better long-term forecasts.
Finally, the ensemble forecast tended to produce sharper forecasts and have a tendency towards under vs over predicting.
There is some suggestion that this was partially driven by the ensemble forecasting having non-exponetial uncertainty, unlike many infectious disease forecasting models. 

Based on these insights from our own forecasting work and from observing the hub ensembles behaviour over time here we define a model with similar, but simplified, epidemiological characteristics to our previous approaches to model based forecasting [@REF] with aim of producing ensemble like perform without sacrificing interpretability.
The first simplification we make is to model only weekly data, rather than using daily data and then aggregating, this means that the impact daily reporting artefacts is mitigated.
It also serves to increase the auto-correlation of the forecasting model as there is an increased lag before changes in daily observations gain signifcant weight in the model this leads to the observed ensemble behaviour of being relatively auto-correlated and resistant to short-term changes in trend.
The second simplification we make is to ignore the underlying latent infection process and focus only on the observed reported cases.
This reduces computational requirements and removes the need for external information on the delay from infection to report which can potentially be misspecified leading to reduced forecast performance.
However, this sacrifices some of the interpretability of the forecast model as any epidemiological summary statistics we now calculate will be based on reported cases and not latent infections.
As discussed in [@gostic] this leads to varying amounts of bias depending on the epidemic phase. 
The final simplification we make is to model the growth rate as a differenced auto-regressive process with order 1 rather than using a more complex method as we have done in other work [@EpiNow2].
This represents a parsiminous approach in that we encode our expectation that the growth rate should vary over time and allow this to influance the forecast but we include only a single lag term and hence reduce the computational overhead of the model.
In order to model potential unobserved interventions and more generally changes in transmission we include an additional growth rate modifier restricted to be between 0 and 1 that differs depending on if the growth rate is positive or negative (due to potential differing responses when cases are growing or increasing) and that acts in a multiplicative fashion (meaning that larger absolute growth rates are reduced to zero growth more rapidly). This reflects a simplified interpretation for how the ensemble appears to react to potential future changes. 
The only observed for which we do not make an adaption is the apparent sharpness of the ensemble compared to our prior forecasting models choosing instead to make use of a negative binomial observation model allowing the inclusion of overdispersion. 
This choice is motivated by our belief that the underlying process is an exponential one and therefore an exponential error model is a sensible choice though this is an area of current research.
We suggest that part of the reason the hub ensembles exhibit this behaviour is due to the penalisation of overprediction compared to underprediction caused by the current usage of a generalsied form of the absolute error for the majority of forecast evaluation. 

In this study, we define an initial attempt at a surrogate model to replicate the observed behaviour of current multi-team forecast ensembles whilst maintaining a set of clear assumptionse and summarise its implementation, with a focus on minimal resource use and reproducibility, as a GitHub action. 
We briefly outline some of the summary statistics it produces and their epidemiological relevance. 
We then evaluate it's real-time performance in comparison to the ECDC forecast hub ensemble by visualising forecasts, using the weighted interval score [@REF], a commonly used proper scoring rule, and quantifying its empirical coverage.
We attempt to highlight settings where this model performs well as a surrogate to the ensemble forecast and areas where more work is needed. 
Finally, we summarise our findings, discuss their implications, and highlight areas for potential study. We aim for this work to highlight some of the potential implicit assumptions of current forecast hub ensembles, provide a sensible, low resource, surrogate model in settings where large-scale colloborative forecasting efforts are not possible, and provide inspiration for forecasters looking to make principled improvements to their models

# Methods {-}

## Data {-}

```{r packages}
library(data.table)
library(scoringutils)
library(lubridate)
library(forcats)
library(ggplot2)
library(ggridges)
library(patchwork)
library(scales)
library(here)
```

```{r load-functions}
source(here("R", "utils.R"))
source(here("R", "evaluate.R"))
source(here("R", "evaluation-plots.R"))
source(here("R", "coverage-plots.R"))
```

```{r data}
# Load JHU frozen truth data. See data-raw/get-truth.R for munging
truth <- fread(here("data", "truth.csv"))

# Load population data
population <- fread(here("data", "population.csv"))

# Load forecast metedata. See data-raw/get-hub-metadata.R for munging
metadata <- fread(here("data", "metadata.csv"))

# Load ECDC forecasts. See data-raw/get-hub-forecasts.R for munging
# Merge with truth and rescale to incidence rate per 100,000
forecasts <- fread(here("data", "forecasts.csv")) |>
  merge_forecasts_with_truth(truth) |>
  rescale_to_incidence_rate(population, scale = 1e4) |>
  rename_models()

# Load forecast scores
scores <- fread(here("data", "scores.csv")) |>
  rename_models()
```
  
  We extracted forecasts and data on notified weekly COVID-19 cases from the ECDC forecasting hub from 15th of January 2022 to the 19th of July 2022 for the ensemble model (referenced as `EuroCOVIDhub-ensemble` by the hub team) and the surrogate model (submitted as `epiforecasts-weeklygrowth`).
  We included all locations covered by the ECDC forecasting hub which were 32 European countries, including all countries of the European Union and European Free Trade Area, and the United Kingdom.
  Data on notified weekly cases was originally sourced from the Johns Hopkins University curated data repository (JHU, [@REF]).
  We used the latest available observed data. As observations are subject to revisions this means that the data used to produce forecasts for a given date may not reflect the data used for evaluation.
  The follows the common practice of the forecasting hub projects in treating the latest available data as the forecast target regardless of the number of data revisions.
  Incidence was aggregated over the Morbidity and Mortality Weekly Report (MMWR) epidemiological week definition of Sunday through Saturday.
  
  The ECDC forecast hub requests forecasts for 1 to 4 week forecast horizon and requires forecasts to use a pre-specified format with 23 quantiles of the predictive probability distribution.
  No restrictions were placed on who could submit forecasts and the hub team actively invited particupation from research groups known to be involved with COVID-19 forecasting projects.
  Forecasters used a wide range of forecasting methods including statistical approaches, mechanistic models including deterministic ordinary differential equation models and agent based model, semi-mechanistic models, expert elicitation models, and ensembles of mutliple approaches (described at https://covid19forecasthub.eu/community.html).
  Teams submitted forecasts at the latest two days after the complete dataset for the latest forecasting week became available and were allowed to use all data available at the time of submission. 
  The ensemble forecast was constructed by taking the median of all predictive quantiles without the exlusion of any valid submitted forecast (where validity was defined as passing minimal formatting checks by the hub team). 
  Submitted forecasts and target observations are freely available with a permissive open-source license from the ECDC forecast hub GitHub repository. We provide code in the repository of this study to streamline access. 

```{r metadata}
# Number of locations
locations <- metadata |>
  DT(, uniqueN(location))

# Number of models overall
models <- metadata |>
  DT(, uniqueN(model))

# Number of forecast dates
forecast_dates <- metadata |>
  DT(, uniqueN(target_end_date))

# Models per forecast date
models_per_forecast <- metadata |>
  summarise_forecasts_by(by = "target_end_date")

# Models per location
models_per_location <- metadata |>
  summarise_forecasts_by(by = "location")

# Models with a location target
locations_per_model <- metadata |>
    summarise_forecasts_by(var = "location", by = "model")

locs_per_model_single_date <- metadata |>
    summarise_forecasts_by(
      var = "location", by = c("model", "target_end_date")
    ) |>
    DT(, unique(.SD[n == max(n), .(n)]), by = "model")

locs_per_model_date <- metadata |>
  summarise_forecasts_by(
    var = "location", by = c("model", "target_end_date")
  ) |>
  DT(locs_per_model_single_date[, .(model, max_n = n)], on = "model") |>
  DT(, n_per := n / max_n) |>
  DT(, .SD[!all(n == max_n)], by = "model")

models_per_loc_date <- metadata |>
  summarise_forecasts_by(var = "model", by = c("target_end_date", "location"))

# Single target models per location
single_locs_models_per_loc <- metadata |>
  DT(locations_per_model[n == 1], on = "model") |>
  summarise_forecasts_by(by = "location")

# Local models
local_models <- locations_per_model |>
  DT(n == 1) |>
  DT(, model)

# Global models
global_models <- locations_per_model |>
  DT(n > 30) |>
  DT(, model)

# Partial coverage models
partial_models <- locations_per_model |>
  DT(n > 2) |>
  DT(n <= 30)

# Varying submissions
varying_submissions <- locs_per_model_date |>
  DT(partial_models[, .(model)], on = "model")

# Models coverage of forecast dats
forecast_dates_by_models <- metadata |>
  summarise_forecasts_by(var = "target_end_date", by = "model") |>
  DT(order(n)) |>
  DT(, n_per := n / forecast_dates) |>
  DT(, type := fcase(
      model %in% global_models, "global",
      model %in% local_models, "local",
      model %in% partial_models[, model], "partial",
      default = "other"
    )
  )
```

## Model {-}

### Definition

We model the expectation ($\lambda_t$) of reported cases ($C_t$) as an order 1 autoregressive (AR(P)) process by epidemiological week ($t$).
The model is initialised by assuming that the initial reported cases are representative with a small amount of error (2.5%).
We assume a negative binomial observation model with overdispersion $\phi$ for reported cases ($C_t$).

\begin{align*}
  \lambda_0 &\sim \text{LogNormal}\left(\log C_0 , 0.025 \times \log C_0 \right)\\
  \lambda_t &= C_{t-1} e^{r_t},\ t > 0  \\
  C_{t} \mid \lambda_{t} &\sim \text{NB}\left(\lambda_t, \phi\right)
\end{align*}

Where $r_t$ can be interpreted as the growth rate. $r_t$ is then modelled as a piecewise constant differenced AR(1) process modified such that the dependence of $r_{t-1}$ is multiplied by a decay factor ($\xi_{+,-}$) that varies dynamically according to the sign of $r_{t-1}$.
The assumptions of this modelling approach are that the growth rate is non-stationary with a trend that is independent of the current growth rate (the differenced AR(1) process), the additional decay factor encodes the belief that larger absolute growth rates will tend more quickly towards no growth and that this process may work differently for positive or negative growth rates.
This process can be defined as follows, 

\begin{align*}
  r_0 &\sim \text{Normal}\left(0, 0.25 \right) \\
  r_t &= \left(\mathcal{H}(r_{t-1} > 0) \xi_{+} + \mathcal{H}(r_{t-1} \le 0) \xi_{-}\right)r_{t-1} + \epsilon_t  \\
  \epsilon_t &= \mathcal{H}(t > 0) \beta \epsilon_{t-1} + \eta_t
\end{align*}

Where $\mathcal{H}$ is the Heaviside step function and is defined such that it attains the value of 1 if the argument is true and the value of 0 otherwise.
The following priors are used,

\begin{align*}
  \xi_{+} &\sim \text{Beta}\left(3, 1 \right) \\
  \xi_{-} &\sim \text{Beta}\left(3, 1 \right) \\
  \beta &\sim \text{Normal}\left(0, 0.25 \right) \\
  \eta_t &\sim \text{Half-Normal}\left(0, \sigma \right) \\
  \sigma &\sim \text{Normal}\left(0, 0.2 \right) \\
  \frac{1}{\sqrt{\phi}} &\sim \text{Half-Normal}(0, 1) 
\end{align*}

Where $\sigma$, and $\frac{1}{\sqrt{phi}}$ are truncated to be greater than 0 and $\beta$ is truncated to be between -1 and 1.
The Beta priors for $\xi{+,-}$ have been chosen to be weakly informative that the reduction towards 0 growth is relatively slow.
Similarly the prior for $\beta$ has been chosen to be weakly informative that there is weak auto-correlation in differenced growth rates.
$\sigma$ has also been made weakly informative under the assumption that the potential change in growth rates in a single timestep should be relatively small.

### Summary statistics

As well as posterior predictions and forecasts for notifcations the model also returns several epidemiological summary statistics which may be useful for drawing inferences about underlying transmission dynamics.
These are the log scale growth rate ($g^{o, \delta}_t$), and the instantaneous effective reproduction number ($R^{o, \delta}_t$).
These are calculated as follows:

\begin{align*}
  g^{o, \delta}_t &= T_s r^{o, \delta}_t \\
  R^{o, \delta}_t &= e^{\left(T_s r^{o, \delta}_t\right)} \\
\end{align*}

$T_s$ is a user set scaling parameter that defines the timespan over which the summary metrics apply dependent on the time step of the data.

## Forecast evaluation {-}

In order to standardise forecasts across forecast locations we first normalised both weekly notified test positive cases and forecast test positive cases by the population in the forecast region scaled to be a rate per 10,000 people.
We then evaluated a subset of forecasts from the following countries Germany, Greece, Italy, Poland, Slovakia, and the United Kingdom visually by forecast horizon (1-4 weeks) for both the ensemble and surrogate model. 

We evaluate forecasts quantatively using the weighted interval score (WIS) [@bracherEvaluatingEpidemicForecasts2021], which is a quantile based proper scoring rule that approximates the e continuous ranked probability score (CRPS).
Both the WIS and CRPS are generalisations of absolute error in order to evaluate probablistic forecasts and are widely used to evaluate COVID-19 forecasts [@REF].
We present WIS for the subset of forecasts we explore visually for both the ensemble and surrogate model by date and forecast horizon (1 and 4 weeks) as well as summarising across the study period.

In order to understand the relative performance of the surrogate model compared to the ensemble model we calculate the relative performance (rWIS) by dividing the WIS for the surrogate model by the WIS of the ensemble model.
In order to maintain the properiety of this score we do this after first taking the means of scores for the relavant stratification.
We explore relative performance by forecast horizon, by forecast month and forecast horizon, and by forecast location and forecast horizon. 

In addition to presenting the WIS for a subset of locations and the relative WIS for all locations we also calculate and visualise the empirical coverage, which is the percentage of observed values within a given interval or below a given quantile, of both the surrogate and ensemble model for the 30%, 60%, and 90% credible intervals and by quantile. 
Lastly we calculate and visualise the relative interval score by quantile, stratified by forecast horizon.

```{r eval-parameters}
locs <- c("United Kingdom", "Germany", "Slovakia", "Italy", "Poland", "Greece")
ranges <- c(30, 60, 90)
```

```{r coverage}
coverage <- calc_coverage(scores)
```

```{r relative-interval-score}
relative_interval_score <- scores |>
  calc_relative_score(
    cols = c("location", "target_end_date", "horizon", "range")
  )
wis <- scores |>
  summarise_scores(
    by = c("location_name", "target_end_date", "horizon", "model")
  )
relative_wis <- wis |>
  calc_relative_score(cols = c("location_name", "target_end_date", "horizon"))
```

## Implementation {-}

The model is implemented in `stan` [@stan] and `R` (`4.2.0`) [@R] as an extension of the baseline model from the `forecast.vocs` R package (`0.0.9.7000`) [@forecast.vocs].
The `cmdstanr` R package (`0.5.2`) [@cmdstanr] is used for model fitting with 2 MCMC chains each having 1000 warm-up and 1000 sampling steps each [@cmdstanr].
`cmstanr` surfaces several settings which trade-off between sampling speed and the robustness of approach.
Here we take a conservative approach, as model fit is not manually inspected during real-time usage and due to the expected complexity of the posterior [@betancourt_2017], and set the adapt delta setting to 0.99, and the maximum treedepth setting to 15. 
For real-time usage convergance was not assessed but during model development the Rhat diagnostic was used alongside feedback from `cmdstanr` about the number of divergant transitions and exceedance of the maximum tree depth. During development posterior predictions were also visually compared to observed data [@cmdstanr].

To download and manipulate forecasts from the ECDC forecasting hub [@EuroHub] we use  the `data.table` (`1.14.2`) [@data.table] and `gh` (`1.3.0`) [@gh] R packages.
We make use of further functionality from the `forecast.vocs` R package [@forecast.vocs] to prepare data for forecasting, to visualise forecasts and summary measures, and to summarise forecasts.
Forecast evaluation is implemented using the `scoringutils` R package (1.0.0) [@scoringutils], and the `scoringRules` R package (1.0.1).

To ensure reproducibility of this analysis dependencies are managed using the `renv` R package (`0.14.0`) [@renv] and a Dockerfile file along with built Docker image [@Boettiger:2015dw] (via GitHub Actions) is provided.
Weekly forecasts are also made use of `renv` and Docker to ensure reproducibility with GitHub actions used to produce the forecasts ensuring we meet our goal of requring limited compute and that our implementation is independent from local resources resulting in democratised access to our approach.
The code for this analysis can be found here: https://github.com/epiforecasts/simplified-forecaster-evaluation
The code for the forecating model defined above along with the infrastructure required to forecast using GitHub Actions can be found here: https://github.com/seabbs/ecdc-weekly-growth-forecasts

# Results {-}

## Data summary {-}

In our study period, incidence rates across ECDC nations and in the UK were primary driven by the spread of noval subvariants of concern related to the Omicron variant. Many nations, such as the UK saw large BA.1 waves in early January to late January that then resulted in declining incidence rates through Febuary. From late Febuary, througth to the end of May, most nations then saw another wave, typically with lower reported incidence rates, driven by BA.2. This wave was typically characterised by a lower peak than the BA.1 wave but a longer tail. The end of our study period was characterised by the gradual take-over of the BA.4/BA.5 subvariants that again had a lower peak and lower absolute growth rates. Unlike earlier periods in the pandemic, our study period did not see the use of new non-pharmaceutical interventions (NPIs) in response to increasing COVID-19 incidence in most locations. In addition, in general ascertainment rates likely reduced over time in most locations due to reductions in routine testing, and reductions in test availability. Whilst both the reduced use of NPIs and testing generally occured across nations our study period also marked an increase in the heterogenity of the response to COVID-19 pandemic with nations changing policy at different times and to different degrees. This is in contrast to the early COVID-19 pandemic response for which most nations took similar actions at similar times.

We extracted forecasts starting from the 15th of January until the 19th of July 2022 for all countries covered by the ECDC forecasting hub (all ECDC nations and the UK making `r locations` unique locations). In total `r nrow(metadata)` forecasts were made across all locations, `r forecast_dates` forecast dates from `r models` independent forecast models (including the ECDC baseline model). Of these models, `r length(global_models)` forecasted in at least 30 locations including our original submission  (referred to as `epiforecasts-EpiNow2` by the hub), and our surrogate model. Of the remaining models submitted `r length(local_models)` were submitted in only one location. These models were gernerally submitted to a subset of locations with particular clustering in Germany and Poland (likely due to the folding of the German/Poland forecasting hub into the ECDC forecasting hub project). Italy was also an outlier with `r single_locs_models_per_loc[location == "IT", n]` models that submitted nowhere else. `r nrow(partial_models)` models submitted for between 3 and 30 locations and all these models varied the number of locations they submitted forecasts for over time, potentially indicating manual curation or models targeted at specific conditions.

Across all forecast dates and locations the minimum number of independent forecasts was `r models_per_loc_date[, min(n)]` with the maxmimum being `r models_per_loc_date[, max(n)]`. The median number of independent forecasts per location and forecast date was `r models_per_loc_date[, median(n)]`. All locations recieved forecasts from at least `r models_per_location[, min(n)]` models with the median number of forecast models per location being `r models_per_location[, median(n)]`. Coverage of forecast dates varied across submitted models with `r forecast_dates_by_models[n_per == 1, .N]` models submitting for all dates, `r forecast_dates_by_models[n_per >= 0.9, .N]` models submitting for at least 90% of dates, and `r forecast_dates_by_models[n_per < 0.50, .N]` models submitting for fewer than 50% of forecast dates. In general, there was no clear difference in forecast date coverage between models that submitted for all locations vs a small subset but models with partial coverage of locations all also had partial coverage of forecast dates.

## Forecast evaluation

### Visualisation of forecasts by horizon {-}

- Forecast performance overview for the ensemble and surrogate.
- Forecast performance overview by forecast horizon. Highlight degrading performance with this impacting the surrogate model to a greater degree. 
- Discussion country level differences in performance and link to potential growth rate trends. 
- Discuss differences between countries with different numbers and kinds of submitted forecast models.

```{r vis-forecasts, fig.cap = "a.) Weighted interval score at the 1 week and 4 week forecast horizon by epidemiological week in Germany, Greece, Italy, Poland, Slovakia, and the United Kingdom. b.)Forecasts of notified test positive cases (per 10,000 population) by epidemiological week in Germany, Greece, Italy, Poland, Slovakia, and the United Kingdom,  by forecast horizon (one to four weeks). Incidence rates are shown on a log scale. 30\\%, 60\\%, and 90\\% credible intervals are shown. The black line and points are the notified cases as of the date of data extraction rather than those available at the time.", out.width = "95%", fig.height = 12, fig.width = 18}
(
  plot_wis(wis, locs) +
   theme(strip.text.x = element_blank()) |
  plot_forecasts(forecasts, locs, ranges) +
    guides(fill = guide_none(), fill_ramp = guide_none(), col = guide_none())
) +
  plot_layout(widths = c(1, 4), guides = "collect") +
  plot_annotation(tag_levels = "a") &
  theme(
    legend.position = "bottom", legend.margin = margin(),
    legend.justification = "centre"
  )
```

### Relative forecast evaluation {-}

- Relative performance of surrogate model by forecast horizon.
   - Overall performance (mean)
   - Give mean relative performance difference by horizon
   - Discuss distribution.
- Relative performance of surrogate model by month stratified by forecast horizon.
  - Give mean relative performance by month for each horizon
  - Discuss distribution
- Relative performance of surrogate model by location sratified by forecast horizon.
  - Discuss qualatative differences between locations

```{r make-rwis-plots}
plot_rwis_horizon <- relative_wis |>
  copy() |>
  DT(, horizon := fct_rev(as.factor(horizon))) |>
  plot_relative_wis(
    y = horizon, fill = horizon,
  ) +
  scale_fill_brewer(palette = "Dark2") +
  scale_color_brewer(palette = "Dark2") +
  labs(y = "Forecast horizon (weeks)") +
  guides(fill = guide_none(), point_color = guide_none(), point = guide_none())

plot_rwis_location <- relative_wis |>
  DT(horizon == 1 | horizon == 4) |>
  plot_relative_wis(
    y = location_name, fill = as.factor(horizon), alpha = 0.3,
    jittered_points = FALSE, quantiles = NULL,  point_color = horizon
  ) +
  scale_fill_brewer(palette = "Dark2") +
  scale_color_brewer(palette = "Dark2") +
  labs(
    y = "Forecast location", fill = "Forecast horizon (weeks)",
    point_color = "Forecast horizon (weeks)"
  )

plot_rwis_by_month <- relative_wis |>
  DT(horizon == 1 | horizon == 4) |>
  DT(, target_month := month(target_end_date, label = TRUE)) |>
  DT(, target_month := fct_rev(target_month)) |>
  plot_relative_wis(
    y = target_month, fill = as.factor(horizon), alpha = 0.3,
    jittered_points = TRUE, quantiles = NULL
  ) +
  scale_fill_brewer(palette = "Dark2") +
  labs(y = "Target forecast month", fill = "Forecast horizon (weeks)") +
  guides(fill = guide_none())
```

```{r assemble-eval, fig.height = 12, fig.width = 12, out.width = "95%"}
 (
  (
    (
      plot_rwis_horizon / plot_rwis_by_month
    )  +
    plot_layout(heights = c(4, 5), guides = "collect")
  ) |
  plot_rwis_location
) +
  plot_layout(guides = "collect", widths = c(1, 1)) +
  plot_annotation(tag_levels = "a") &
  theme(
    legend.position = "bottom", legend.margin = margin(),
    legend.justification = "centre"
  )
```

### Forecast calibration {-}

- Calibration of forecasts at credible ranges of interest.
- Calibration across quantiles
- Relative interval scores by quantile

```{r plot-coverage, fig.height = 12, fig.width = 18, out.width = "95%"}
 (
  (
    (
      plot_coverage_range(coverage, ranges) /
      plot_rel_score_by_quantile(relative_interval_score)
    )  +
    plot_layout(heights = c(3, 1), guides = "collect")
  ) |
  plot_coverage_quantiles(scores)
) +
  plot_layout(widths = c(1, 2), guides = "collect") +
  plot_annotation(tag_levels = "a") &
  theme(
    legend.position = "bottom", legend.margin = margin(),
    legend.justification = "centre"
  )
```

# Discussion {-}

## Summary {-}

In this study, we defined a surrogate model aiming to replicate some of the observed behaviour of the ECDC forecast hub multi-team ensemble for forecasting test positive reported COVID-19 cases in ECDC nations and the UK. We first derived a set of assumptions for how the surrogate model should behaved based on our observations of the ECDC forecast hub ensemble, our experiance submitting forecasts to various forecast Hubs and our observations of the performance of other models. We restricted ourselves to a model that could be easily understand, that produced meaningful summary statistics, and that could be run with low compute resources. We further provide a fully reproducibile workflow for running this model using GitHub actions and code to evaluate its performance. 

Over the 6 months of the study period, we found that our surrogate model performed adequately at reproducing the forecast hub ensemble with average performance that was in general slightly worse though this varied by location and forecast date. Performance was closest for short forecast horizons and degraded relative to the forecast hub ensemble as the horizon increased. We observed that our surrogate model displayed similar levels of robustness to the forecast ensemble with few outlier forecasts unlike our previous forecasting efforts [@refNikosinGermany] and also partially captured the hub ensembles propensity to forecast reductions in incidence after periods of increase. Unlike the forecast hub ensemble our surrogate model was well calibrated at all horizons and credible intervals explored though it was overly uncertain at short horizons. We found that the majority of the difference in performance between our surrogate model and the hub ensemble occured in the tails of the predictive distribution with similar central forecasts.

In this study, we define an initial attempt at a surrogate model to replicate the observed behaviour of current multi-team forecast ensembles whilst maintaining a set of clear assumptionse and summarise its implementation, with a focus on minimal resource use and reproducibility, as a GitHub action. 
We briefly outline some of the summary statistics it produces and their epidemiological relevance. 
We then evaluate it's real-time performance in comparison to the ECDC forecast hub ensemble by visualising forecasts, using the weighted interval score [@REF], a commonly used proper scoring rule, and quantifying its empirical coverage.
We attempt to highlight settings where this model performs well as a surrogate to the ensemble forecast and areas where more work is needed. 
Finally, we summarise our findings, discuss their implications, and highlight areas for potential study. We aim for this work to highlight some of the potential implicit assumptions of current forecast hub ensembles, provide a sensible, low resource, surrogate model in settings where large-scale colloborative forecasting efforts are not possible, and provide inspiration for forecasters looking to make principled improvements to their models


## Stengths and weaknesses {-}

- Real-time evaluation of a model with development tracked using version control (showing no changes since January 2022).
- Conducted using reproducibility best practices with all code available for reuse. THe model is implemented in GitHub actions so can be reused without any compute resources and is by definition fast to run.
- Only use absolute scoring which has implications for what we consider a good model. This is the approach used by the hub and so makes sense here but area of active research to understand what is more appropriate.
- Limited exploration of model formulations. Likely that there are alternatives that perform closer to the ensemble
- As our target is not to produce the best COVID-19 forecast model, or the best COVID-19 forecast model as defined using the metrics used to evaluate by the ECDC but rather to develop a model that closely matches the Hub ensemble whilst maintaining interpretability and using limited resources it is likely that it will perform less  well than a forecast model designed with the first two criterion in mind. 
  - However, if we start from the review that the forecast hub ensemble has traits that are desirable for use by policy-makers (i.e robustness and good average performance) then our approach makes sense. This is the view often used in the literature.
- As the majority of the difference in relative performance occurred in the tail of the predictive distribution and that performance in the central predictive distribution was fairly similar this indicates that fore post-processing could make performance more similar. However, this would like reduce the calibration of our surrogate model which is not to be desired. Instead we hypothesis that this feature is driven by potentially inappropriate use of absolute scoring methods for exponential processes though our choice here was motivated by the methodology adopted by the forecast hubs.

## Literature context {-}

  - Emulators in weather forecasting and elsewhere
  - ECDC and CDC ensemble papers
  - Other forecasting models contributed to the ensembles
  - Baseline models

## Further work {-}

Whilst we found that our surrogate model performed compab
  - Other similar model formulations. More lag terms. Reduced run-times using approximate Bayesian approaches.
  - Role of simple models for adjusting for susceptible population. Highlight useful now but not in early pandemic
  - Follow up study comparing these results to retrospective forecasts from early in the pandemic or on other infectious disease outbreaks (though limited by requirement of a multi-team ensemble to compare to).
  
## Conclusions {-}

# Acknowledgments {-}

# References
