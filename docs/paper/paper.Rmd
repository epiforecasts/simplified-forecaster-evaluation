---
title: Evaluating a simplified forecast model in comparison to the ECDC forecasting hub ensemble
preprint: false
author: 
  - name: Sam Abbott
    affiliation: 1
    corresponding: true
    email: sam.abbott@lshtm.ac.uk
affiliation:
  - code: 1
    address: Cold Spring Harbor Laboratory, One Bungtown Road Cold Spring Harbor, NY 11724
abstract: > 
  We made the best model. It is both simple and amazing.
bibliography: library.bib
output:
  rticles::peerj_article: default
  bookdown::pdf_book:
    base_format: rticles::peerj_article
    extra_dependencies: ["float"]
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = TRUE,
  echo = FALSE,
  message = FALSE,
  cache = TRUE,
  messages = FALSE,
  warnings = FALSE
)
```

# Introduction {-}

Multi-model and  multi-team ensembles have become increasingly popular as an approach to increase the robustness and performance of infectious disease forecasts over the last decade with significant success [@REF].
These approaches have drawn heavily on the experiance of other domains, for example climate modelling [@REF], where ensembles of both multiple models and from multiple teams have a long history of providing superior forecasts.
However, ensemble forecasts have range of potential downsides with the most obvious being their considerable cost to produce as they typically require contributions from multiple independent teams, the development of several models, and a centralised group to run the ensembling project, and that forecasts from these ensembles can be difficult to link to the underling model assumption.
In climate forecasting emulation approaches have been used to circumvent some of the  resource requirment issue by training a simplified model, usually a non-parametric statistical model, to replicate the behaviour of either the entire model or sub-components [@REF].
However, within epidemiological modelling we often want to use our models for applications other than forecasting, have fewer insights beyond forecast performance on which to base our mechanistic assumptions. 
This means that non-parametric emulation is not an ideal solution. 
In this work we propose and evaluate a surrogate forecast model approach which seeks to emulate the observed behaviour of recent COVID-19 forecast ensembles using a minimal set of easily communicated assumptions, limited computational resources, and an easily generalised implementation. 

The trend towards large-scale multi-team ensemble forecasting in infectious diseases has accelerated during the COVID-19 pandemic, partially due to an increased need from stakeholders for reliable forecast models but also due to the percieved low quality of many forecasts otherwise being produced, with the establishment of the CDC forecasting hub [@REF] in early 2020, the Germany/Poland forecast hub [@REF] in late 2020, and ECDC forecast hub [@ECDC] in 2021. All of these collaborations, which the authors (particularly SF and JB have been involved with managing),  ensemble contributions from multiple independent teams using a similar ensembling approach (most recently an unweighted median ensemble [@REF]) and have shown that their ensemble forecasts outperform most individually contributed forecasts and provide forecasts that are more robust to outliers [@REF].
However, they have struggled to provide feedback to those contributing forecasts that would allow them to improve their forecast approaches, have found it difficult to improve the quality of their ensemble forecasts [@REF], and have struggled to maintain incentives for forecasters to continue to contribute which may impact the long-term quality of the  ensembles they produce. 

These forecast collaborations, and other similar infectious disease forecasting efforts, have focussed on forecasting reported cases and deaths by epidemiological week (i.e Saturday to Saturday) over a 1 to 4 week time horizon. Generally ground truth data is available daily, and is updated in retrospect as new data becomes available. Typically, the ensemble forecast produced each week is made up of at least 5 independent forecasts using a range of approaches and is produced using an unweighted median ensemble [@REF].
The majority of submitted forecasts make use of the available daily data, along with potentially other leading predictors to produce daily forecasts that are then aggregated to weekly forecasts.
Many use a range of outlier processing techniques to reduce reporting artefacts which can bias forecasts.
In general, 4 main classes of forecast models are used, statistical forecasting models such as ARIMA models [@REF], mechanistic forecasting models based on the compartmental modelling framework and it's generalisations [@REF], semi-mechanistic approaches that blend both of these approaches [@REF], and human insight based forecast models that may also include elements of other methods [@REF].
Real-time evaluation has shown that each of these classes of models may perform well though in general semi-mechanistic models, and models that include human insights have outperformed other approaches during certain periods of time [@REF].
We have contributed a range of forecasts to these forecast collobarations generally focussed on semi-mechanistic statistical methods [@REF], and human insight based forecasts [@REF] with our forecasts performing relatively well compared to the majority of those submitted but rarely outperforming significantly.
The model based forecasts we have contributed have focussed on trying to carefully model the underlying infectious disease dynamics from infection through to symptom onset, and test positivity using non-exponential delay distributions whilst also attempting to model the complexity of daily, within week, reporting periodicity [@REF].
Our forecasts have generally captured changes in trend better than most submitted models that do not use human insight based on known policy changes but have not been robust to reporting issues such as large outliers in reporting and changes to reporting patterns. 
Our previous methodology also requires signinficant computational resources running for an hour on a 16 core machine when producing forecasts from the ECDC. 
In our model based forecasts we did not attempt to capture potential future interventions or known interventions not currently observed in the epidemiological data whereas in our human insight models these were implicitly included.
We found that our human insight based forecasts significantly outperformed our model based forecasts and hypothesised that this may have been driven by including additional information not observed in the epidemiological data [@REF]. 

- Extension of why
   - Brief discussion of EpiNow2 and related models
   - Brief dicussion of statistical modelling approaches (use MUNI-ARIMA as an example).
   - Brief discussion of scoring of hub models
   - Summary of observed performance of the hub ensembles
    - Auto-correlated
    - Typically relatively trend following though shows some ability to forecast future changes. 

Based on these insights here we define a simplified auto-regressive weekly growth rate model that assumes a non-stationary trend in growth rate. This means we do not need to model daily reporting processes, which can cause severe performance issues due to reporting artefacts
- What
  - Simplified auto-regressive growth rate model assuming a non-stationary trend
  -  Explicit modelling of the assumption that rapid growth/decay of cases will typically result in future growth tending towards zero. This seeks to capture potential implementation and relaxation of NPIs as well as individuals attitudes to risk (assuming this is based on current growth rates and not case incidence).
  - Uses weekly vs daily data to side step issues of reporting periodicity and to replicate the observed auto-corelation of the hub ensembles.
  - Simple extension of an already implemented model and resuse of its modular components.
  - Implemented with a focus on reproducibility, simplicity, and low resource use. 
  - The model runs using GitHub actions in a framework supplied by the ECDC forecast hub. We provide our own tools so that others can run the model using GitHub actions democratising our approach.

- Aim: In this work we define a simplified forecasting model designed to replicate some of the observed characteristics of successful infectious disease ensemble models without sacrificing interpretability and with low resource requirements. We evaluate the performance of this model against the ECDC forecasting hub ensemble in real-tim, and discuss implications of our findings along with areas for future research.

# Methods {-}

## Data {-}

```{r packages}
library(data.table)
library(scoringutils)
library(lubridate)
library(forcats)
library(ggplot2)
library(ggridges)
library(patchwork)
library(scales)
library(here)
```

```{r load-functions}
source(here("R", "utils.R"))
source(here("R", "evaluate.R"))
source(here("R", "evaluation-plots.R"))
source(here("R", "coverage-plots.R"))
```

```{r data}
# Load JHU frozen truth data. See data-raw/get-truth.R for munging
truth <- fread(here("data", "truth.csv"))

# Load population data
population <- fread(here("data", "population.csv"))

# Load ECDC forecast metadata. See data-raw/get-hub-metadata.R for munging
#metadata <- fread(here("data", "forecast-metadata.csv"))

# Load ECDC forecasts. See data-raw/get-hub-forecasts.R for munging
# Merge with truth and rescale to incidence rate per 100,000
forecasts <- fread(here("data", "forecasts.csv")) |>
  merge_forecasts_with_truth(truth) |>
  rescale_to_incidence_rate(population, scale = 1e4) |>
  rename_models()

# Load forecast scores
scores <- fread(here("data", "scores.csv")) |>
  rename_models()
```

- Data from JHU as used as truth data by the ECDC forecasting hub.
  - When accessed
  - Aggregated to epiweek as in the ECDC forecasting GitHub
  - Countries included
  - Dates of data included
- Forecasts from the ECDC forecast hub
  - Forecasts dowloaded for the model proposed here and the ECDC ensemble
  - Forecast format
  - Forecases from when and horizon
  - Ensemble construction and reference
  
## Model {-}

### Motivation

The same underlying structure with expected notifications being assumed to be a product of past expected notifications, and an exponentiated time-varying term.
This term is then modelled in both instances as a differenced autoregressive process with a single lag term.
By default it is assumed to vary inline with the timestep of the data but this can be altered to speed up computation or to improve out of sample performance.
Observed cases are estimated from expected cases by assuming a negative binomial observation model.
A detailed model definition is given below.

### Definition

We model the expectation ($\lambda_t$) of reported cases ($C_t$) as an order 1 autoregressive (AR(P)) process by epidemiological week ($t$).
The model is initialised by assuming that the initial reported cases are representative with a small amount of error (2.5%).
We assume a negative binomial observation model with overdispersion $\phi$ for reported cases ($C_t$).

\begin{align*}
  \lambda_0 &\sim \text{LogNormal}\left(\log C_0 , 0.025 \times \log C_0 \right)\\
  \lambda_t &= C_{t-1} e^{r_t},\ t > 0  \\
  C_{t} \mid \lambda_{t} &\sim \text{NB}\left(\lambda_t, \phi\right)
\end{align*}

Where $r_t$ can be interpreted as the growth rate. $r_t$ is then modelled as a piecewise constant differenced AR(1) process modified such that the dependence of $r_{t-1}$ is multiplied by a decay factor ($\xi_{+,-}$) that varies dynamically according to the sign of $r_{t-1}$.
The assumptions of this modelling approach are that the growth rate is non-stationary with a trend that is independent of the current growth rate (the differenced AR(1) process), the additional decay factor encodes the belief that larger absolute growth rates will tend more quickly towards no growth and that this process may work differently for positive or negative growth rates.
This process can be defined as follows, 

\begin{align*}
  r_0 &\sim \text{Normal}\left(0, 0.25 \right) \\
  r_t &= \left(\mathcal{H}(r_{t-1} > 0) \xi_{+} + \mathcal{H}(r_{t-1} \le 0) \xi_{-}\right)r_{t-1} + \epsilon_t  \\
  \epsilon_t &= \mathcal{H}(t > 0) \beta \epsilon_{t-1} + \eta_t
\end{align*}

Where $\mathcal{H}$ is the Heaviside step function and is defined such that it attains the value of 1 if the argument is true and the value of 0 otherwise.
The following priors are used,

\begin{align*}
  \xi_{+} &\sim \text{Beta}\left(3, 1 \right) \\
  \xi_{-} &\sim \text{Beta}\left(3, 1 \right) \\
  \beta &\sim \text{Normal}\left(0, 0.25 \right) \\
  \eta_t &\sim \text{Half-Normal}\left(0, \sigma \right) \\
  \sigma &\sim \text{Normal}\left(0, 0.2 \right) \\
  \frac{1}{\sqrt{\phi}} &\sim \text{Half-Normal}(0, 1) 
\end{align*}

Where $\sigma$, and $\frac{1}{\sqrt{phi}}$ are truncated to be greater than 0 and $\beta$ is truncated to be between -1 and 1.
The Beta priors for $\xi{+,-}$ have been chosen to be weakly informative that the reduction towards 0 growth is relatively slow.
Similarly the prior for $\beta$ has been chosen to be weakly informative that there is weak auto-correlation in differenced growth rates.
$\sigma$ has also been made weakly informative under the assumption that the potential change in growth rates in a single timestep should be relatively small.

### Summary statistics

As well as posterior predictions and forecasts for notifcations the model also returns several epidemiological summary statistics which may be useful for drawing inferences about underlying transmission dynamics.
These are the log scale growth rate ($g^{o, \delta}_t$), and the instantaneous effective reproduction number ($R^{o, \delta}_t$).
These are calculated as follows:

\begin{align*}
  g^{o, \delta}_t &= T_s r^{o, \delta}_t \\
  R^{o, \delta}_t &= e^{\left(T_s r^{o, \delta}_t\right)} \\
\end{align*}

$T_s$ is a user set scaling parameter that defines the timespan over which the summary metrics apply dependent on the time step of the data.

## Forecast evaluation {-}

In order to standardise forecasts across forecast locations we first normalised both weekly notified test positive cases and forecast test positive cases by the population in the forecast region scaled to be a rate per 10,000 people.
We then evaluated a subset of forecasts from the following countries Germany, Greece, Italy, Poland, Slovakia, and the United Kingdom visually by forecast horizon (1-4 weeks) for both the ensemble and surrogate model. 

We evaluate forecasts quantatively using the weighted interval score (WIS) [@bracherEvaluatingEpidemicForecasts2021], which is a quantile based proper scoring rule that approximates the e continuous ranked probability score (CRPS).
Both the WIS and CRPS are generalisations of absolute error in order to evaluate probablistic forecasts and are widely used to evaluate COVID-19 forecasts [@REF].
We present WIS for the subset of forecasts we explore visually for both the ensemble and surrogate model by date and forecast horizon (1 and 4 weeks) as well as summarising across the study period.

In order to understand the relative performance of the surrogate model compared to the ensemble model we calculate the relative performance (rWIS) by dividing the WIS for the surrogate model by the WIS of the ensemble model.
In order to maintain the properiety of this score we do this after first taking the means of scores for the relavant stratification.
We explore relative performance by forecast horizon, by forecast month and forecast horizon, and by forecast location and forecast horizon. 

In addition to presenting the WIS for a subset of locations and the relative WIS for all locations we also calculate and visualise the empirical coverage, which is the percentage of observed values within a given interval or below a given quantile, of both the surrogate and ensemble model for the 30%, 60%, and 90% credible intervals and by quantile. 
Lastly we calculate and visualise the relative interval score by quantile, stratified by forecast horizon.

```{r eval-parameters}
locs <- c("United Kingdom", "Germany", "Slovakia", "Italy", "Poland", "Greece")
ranges <- c(30, 60, 90)
```

```{r coverage}
coverage <- calc_coverage(scores)
```

```{r relative-interval-score}
relative_interval_score <- scores |>
  calc_relative_score(
    cols = c("location", "target_end_date", "horizon", "range")
  )
wis <- scores |>
  summarise_scores(
    by = c("location_name", "target_end_date", "horizon", "model")
  )
relativ_wis <- wis |>
  calc_relative_score(cols = c("location_name", "target_end_date", "horizon"))
```

## Implementation {-}

The model is implemented in `stan` [@stan] and `R` (`4.2.0`) [@R] as an extension of the baseline model from the `forecast.vocs` R package (`0.0.9.7000`) [@forecast.vocs].
The `cmdstanr` R package (`0.5.2`) [@cmdstanr] is used for model fitting with 2 MCMC chains each having 1000 warm-up and 1000 sampling steps each [@cmdstanr].
`cmstanr` surfaces several settings which trade-off between sampling speed and the robustness of approach.
Here we take a conservative approach, as model fit is not manually inspected during real-time usage and due to the expected complexity of the posterior [@betancourt_2017], and set the adapt delta setting to 0.99, and the maximum treedepth setting to 15. 
For real-time usage convergance was not assessed but during model development the Rhat diagnostic was used alongside feedback from `cmdstanr` about the number of divergant transitions and exceedance of the maximum tree depth. During development posterior predictions were also visually compared to observed data [@cmdstanr].

To download and manipulate forecasts from the ECDC forecasting hub [@EuroHub] we use  the `data.table` (`1.14.2`) [@data.table] and `gh` (`1.3.0`) [@gh] R packages.
We make use of further functionality from the `forecast.vocs` R package [@forecast.vocs] to prepare data for forecasting, to visualise forecasts and summary measures, and to summarise forecasts.
Forecast evaluation is implemented using the `scoringutils` R package (1.0.0) [@scoringutils], and the `scoringRules` R package (1.0.1).

To ensure reproducibility of this analysis dependencies are managed using the `renv` R package (`0.14.0`) [@renv] and a Dockerfile file along with built Docker image [@Boettiger:2015dw] (via GitHub Actions) is provided.
Weekly forecasts are also made use of `renv` and Docker to ensure reproducibility with GitHub actions used to produce the forecasts ensuring we meet our goal of requring limited compute and that our implementation is independent from local resources resulting in democratised access to our approach.
The code for this analysis can be found here: https://github.com/epiforecasts/simplified-forecaster-evaluation
The code for the forecating model defined above along with the infrastructure required to forecast using GitHub Actions can be found here: https://github.com/seabbs/ecdc-weekly-growth-forecasts

# Results {-}

## Data summary {-}

- Date range of forecasts
- Forecasts per country
- Total number of forecast models and average per location and per forecast date
- Summarises general trends in incidence in the data

## Forecast evaluation

### Visualisation of forecasts by horizon {-}

- Forecast performance overview for the ensemble and surrogate.
- Forecast performance overview by forecast horizon. Highlight degrading performance with this impacting the surrogate model to a greater degree. 
- Discussion country level differences in performance and link to potential growth rate trends. 

```{r vis-forecasts, fig.cap = "a.) Weighted interval score at the 1 week and 4 week forecast horizon by epidemiological week in Germany, Greece, Italy, Poland, Slovakia, and the United Kingdom. b.)Forecasts of notified test positive cases (per 10,000 population) by epidemiological week in Germany, Greece, Italy, Poland, Slovakia, and the United Kingdom,  by forecast horizon (one to four weeks). Incidence rates are shown on a log scale. 30\\%, 60\\%, and 90\\% credible intervals are shown. The black line and points are the notified cases as of the date of data extraction rather than those available at the time.", out.width = "95%", fig.height = 12, fig.width = 18}
(
  plot_wis(wis, locs) +
   theme(strip.text.x = element_blank()) |
  plot_forecasts(forecasts, locs, ranges) +
    guides(fill = guide_none(), fill_ramp = guide_none(), col = guide_none())
) +
  plot_layout(widths = c(1, 4), guides = "collect") +
  plot_annotation(tag_levels = "a") &
  theme(
    legend.position = "bottom", legend.margin = margin(),
    legend.justification = "centre"
  )
```

### Relative forecast evaluation {-}

- Relative performance of surrogate model by forecast horizon.
   - Give mean relative performance difference by horizon
   - Discuss distribution.
- Relative performance of surrogate model by month stratified by forecast horizon.
  - Give mean relative performance by month for each horizon
  - Discuss distribution
- Relative performance of surrogate model by location sratified by forecast horizon.
  - Discuss qualatative differences between locations

```{r make-rwis-plots}
plot_rwis_horizon <- relative_wis |>
  copy() |>
  DT(, horizon := fct_rev(as.factor(horizon))) |>
  plot_relative_wis(
    y = horizon, fill = horizon,
  ) +
  scale_fill_brewer(palette = "Dark2") +
  scale_color_brewer(palette = "Dark2") +
  labs(y = "Forecast horizon (weeks)") +
  guides(fill = guide_none(), point_color = guide_none(), point = guide_none())

plot_rwis_location <- relative_wis |>
  DT(horizon == 1 | horizon == 4) |>
  plot_relative_wis(
    y = location_name, fill = as.factor(horizon), alpha = 0.3,
    jittered_points = FALSE, quantiles = NULL,  point_color = horizon
  ) +
  scale_fill_brewer(palette = "Dark2") +
  scale_color_brewer(palette = "Dark2") +
  labs(
    y = "Forecast location", fill = "Forecast horizon (weeks)",
    point_color = "Forecast horizon (weeks)"
  )

plot_rwis_by_month <- relative_wis |>
  DT(horizon == 1 | horizon == 4) |>
  DT(, target_month := month(target_end_date, label = TRUE)) |>
  DT(, target_month := fct_rev(target_month)) |>
  plot_relative_wis(
    y = target_month, fill = as.factor(horizon), alpha = 0.3,
    jittered_points = TRUE, quantiles = NULL
  ) +
  scale_fill_brewer(palette = "Dark2") +
  labs(y = "Target forecast month", fill = "Forecast horizon (weeks)") +
  guides(fill = guide_none())
```

```{r assemble-eval, fig.height = 12, fig.width = 12, out.width = "95%"}
 (
  (
    (
      plot_rwis_horizon / plot_rwis_by_month
    )  +
    plot_layout(heights = c(4, 5), guides = "collect")
  ) |
  plot_rwis_location
) +
  plot_layout(guides = "collect", widths = c(1, 1)) +
  plot_annotation(tag_levels = "a") &
  theme(
    legend.position = "bottom", legend.margin = margin(),
    legend.justification = "centre"
  )
```

### Forecast calibration {-}

- Calibration of forecasts at credible ranges of interest.
- Calibration across quantiles
- Relative interval scores by quantile

```{r plot-coverage, fig.height = 12, fig.width = 18, out.width = "95%"}
 (
  (
    (
      plot_coverage_range(coverage, ranges) /
      plot_rel_score_by_quantile(relative_interval_score)
    )  +
    plot_layout(heights = c(3, 1), guides = "collect")
  ) |
  plot_coverage_quantiles(scores)
) +
  plot_layout(widths = c(1, 2), guides = "collect") +
  plot_annotation(tag_levels = "a") &
  theme(
    legend.position = "bottom", legend.margin = margin(),
    legend.justification = "centre"
  )
```

# Discussion {-}

## Summary {-}

## Stengths and weaknesses {-}

## Literature context {-}

  - Emulators in weather forecasting and elsewhere
  - ECDC and CDC ensemble papers
  - Other forecasting models contributed to the ensembles
  - Baseline models

## Further work {-}

  - Other similar model formulations. More lag terms. Reduced run-times using approximate Bayesian approaches.
  
## Conclusions {-}

# Acknowledgments {-}

# References
