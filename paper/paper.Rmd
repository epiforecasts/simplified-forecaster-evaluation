---
title: Evaluating a simplified forecast model in comparison to the ECDC forecasting hub ensemble
preprint: true
author: 
  - name: Sam Abbott
    affiliation: 1
    corresponding: true
    email: sam.abbott@lshtm.ac.uk
  - name: Katharine Sherratt
    affiliation: 1
  - name: Nikos Bosse
    affiliation: 1
  - name: Hugo Gruson
    affiliation: 1
  - name:  Johannes Bracher
    affiliation: 2
  - name: Sebastian Funk
    affiliation: 1
affiliation:
  - code: 1
    address: London School of Hygiene and Tropical Medicine, London, UK
  - code: 2
    address: Chair of Statistics and Econometrics, Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany
abstract: > 
  Multi-model and multi-team ensembles have become increasingly popular as an approach to increase the robustness and performance of infectious disease forecasts, especially during the COVID-19 pandemic. However, these forecasts require a coordinated team to produce and are effectively black boxes. In other fields, resource usage has been reduced by training simplified models that reproduce some of the observed behaviour of more complex models. Here we use observations of the behaviour of the ECDC forecast ensemble, combined with our own forecasting experience,  to specify a series of model assumptions. We then develop a forecast model using these assumptions and compare its performance to the ECDC forecast hub ensemble in real-time from the 15th of January 2022 to the 19th of July 2022 over a four-week forecast horizon. We found that this surrogate model performed visibly similarly to a subset of example locations though with increased uncertainty. Unsurprisingly, using the weighted interval score, we concluded that average performance was substantially worse but that median performance was more comparable. The majority of the difference in performance was linked to a few forecasts from a period of high incidence. We note that despite performing worse than the hub ensemble the majority of the time our surrogate model was better calibrated, and less liable to underpredict. The central part of the surrogate models forecast distribution performed more similarly to ensemble forecasts distribution whilst the tails of the surrogate forecast distribution were responsible for the large majority of the difference in median forecast performance. We conclude that our simplified forecast model may have captured some of the dynamics of the hub ensemble but that more work needs to be done to understand the implicit epidemiological model that represents its behaviour and whether or not this is the optimal choice for stakeholders' requirements. We also conclude that our findings are largely driven by the choice of evaluation measure used by the forecast hub and that if this is misspecified then the forecast hub submissions may not be optimised to forecast users' expectations or requirements. Our work is potentially useful for forecast consumers as a tool for understanding the inherent assumptions of the forecasts they are making use of and to researchers developing forecast approaches that perform similarly, or better than, current multi-model and multi-team forecast ensembles that are trusted by stakeholders.
bibliography: library.bib
output:
  bookdown::pdf_book:
    base_format: rticles::peerj_article
    extra_dependencies: ["float"]
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = TRUE,
  echo = FALSE,
  message = FALSE,
  cache = TRUE,
  messages = FALSE,
  warnings = FALSE
)
```

# Introduction {-}

Multi-model and multi-team ensembles have become increasingly popular as an approach to increase the robustness and performance of infectious disease forecasts over the last decade with significant success [@REF].
These approaches have drawn heavily on the experience of other domains, for example, climate modelling [@REF], where ensembles of both multiple models and from multiple teams have a long history of providing superior forecasts.
However, ensemble forecasts have a range of potential downsides with the most obvious being their considerable cost to produce as they typically require contributions from multiple independent teams, the development of several models, and a centralised group to run the ensembling project, and that forecasts from these ensembles can be difficult to link to the underlying model assumption.
In climate forecasting emulation approaches have been used to circumvent some of the resource requirement issues by training a simplified model, usually, a non-parametric statistical model, to replicate the behaviour of either the entire model or sub-components [@REF].
However, within epidemiological modelling, we often want to use our models for applications other than forecasting and have fewer insights beyond forecast performance on which to base our mechanistic assumptions. 
This means that non-parametric emulation is less suitable for use in epidemiological forecasting though for epidemiological scenario analysis in-sample emulation using non-parametric methods may have a role [@REFLSHTMTBEMULATOR].
In this work, we propose and evaluate a surrogate forecast model approach which seeks to emulate the observed behaviour of recent COVID-19 forecast ensembles using a minimal set of easily communicated assumptions, limited computational resources, and an easily generalised implementation. 

The trend towards large-scale multi-team ensemble forecasting in infectious diseases has accelerated during the COVID-19 pandemic, partially due to an increased need from stakeholders for reliable forecast models but also due to the perceived low quality of many forecasts otherwise being produced, with the establishment of the CDC forecasting hub [@REF] in early 2020, the Germany/Poland forecast hub [@REF] in late 2020, and ECDC forecast hub [@ECDC] in 2021. All of these collaborations, which the authors (particularly SF and JB have been involved with managing),  ensemble contributions from multiple independent teams using a similar ensembling approach (most recently an unweighted median ensemble [@REF]) and have shown that their ensemble forecasts outperform most individually contributed forecasts whilst remaining generally robust to outliers in reporting [@REF].
However, they have struggled to provide feedback to those contributing forecasts that would allow them to improve their forecast approaches, have found it difficult to improve the quality of their ensemble forecasts [@REF], and have struggled to maintain incentives for forecasters to continue to contribute which may impact the long-term quality of the ensembles they produce. 

These forecast collaborations, and other similar infectious disease forecasting efforts, have focussed on forecasting reported cases and deaths by epidemiological week (i.e Saturday to Saturday) over a one to four-week time horizon. Generally, ground truth data is available daily, and is updated in retrospect as new data becomes available. Typically, the ensemble forecast produced each week comprises at least 5 independent forecasts using a range of approaches and is produced using an unweighted median ensemble [@REF].
Most submitted forecasts use available daily data, along with potentially other leading predictors, to produce daily forecasts that are then aggregated into weekly forecasts.
Many use a range of outlier processing techniques to reduce reporting artefacts which can bias forecasts.
In general, 4 main classes of forecast models are submitted, statistical forecasting models such as ARIMA models [@REF], mechanistic forecasting models based on the compartmental modelling framework and its generalisations [@REF], semi-mechanistic approaches that blend both of these approaches [@REF], and human insight based forecast models that may also include elements of other methods [@REF].
Real-time evaluation has shown that each of these classes of models may perform well though in general semi-mechanistic models, and models that include human insights have outperformed other approaches during certain periods [@REF].
We have contributed a range of forecasts to these forecast collaborations generally focussed on semi-mechanistic statistical methods [@REF], and human insight-based forecasts [@REF] with our forecasts performing relatively well compared to the majority of those submitted but rarely outperforming significantly.
The model-based forecasts we have contributed have focussed on trying to carefully model the underlying infectious disease dynamics from infection through to symptom onset, and test positivity using non-exponential delay distributions whilst also attempting to model the complexity of daily, within the week, reporting periodicity [@REF].
Based on our observations our forecasts have generally captured changes in trend better than most submitted models that do not use human insight based on known policy changes but have not been robust to reporting issues such as large outliers in reporting and changes to reporting patterns. 
Our previous methodology also requires significant computational resources running for an hour on a 16-core machine when producing forecasts for the ECDC forecasting hub. 
In our model-based forecasts, we did not attempt to capture potential future interventions or known interventions not currently observed in the epidemiological data whereas in our human insight models these were implicitly included.
We found that our human insight-based forecasts significantly outperformed our model-based forecasts and hypothesised that this may have been driven by including additional information not observed in the epidemiological data [@REF]. 
Unlike our epidemiologically motivated forecast submissions, the hub ensemble forecasts were typically robust to daily reporting artefacts.
They also demonstrated some ability to forecast future changes in trends that were not present in the observed data similarly to our human insight forecasts indicating the likely inclusion of either human insight, or assumptions about future interventions. 
In general, the ensemble forecasts were more auto-correlated and less reactive to changes from stable or reducing case incidence to increasing incidence.
On the other hand, this also meant that the ensemble was less likely to adopt short-term changes in incidence and hence produced better long-term forecasts.
Finally, the ensemble forecast tended to produce sharper forecasts and have a tendency towards under vs over predicting.
There is some suggestion that this was partially driven by the ensemble forecast having non-exponential uncertainty, unlike many infectious disease forecasting models. 

Based on these insights from observing the performance of our forecast submissions and from the hub ensembles over time here we define a model with similar, but simplified, epidemiological characteristics to our previous approaches to model-based forecasting [@REF] with aim of producing ensemble-like performance without sacrificing interpretability.
The first simplification we make is to model only weekly data, rather than using daily data and then aggregating, this means that the impact of daily reporting artefacts is mitigated.
It also serves to increase the auto-correlation of the forecasting model as there is an increased lag before changes in daily observations gain significant weight in the model this leads to the observed ensemble behaviour of being relatively auto-correlated and resistant to short-term changes in trend.
The second simplification we make is to ignore the underlying latent infection process and focus only on the observed reported cases.
This reduces computational requirements and removes the need for external information on the delay from infection to report which can potentially be misspecified leading to potentially reduced forecast performance.
However, this sacrifices some of the interpretability of the forecast model as any epidemiological summary statistics we now calculate will be based on reported cases and not latent infections.
As discussed in [@gostic] this leads to varying amounts of bias depending on the epidemic phase. 
The final simplification we make is to model the growth rate as a differenced auto-regressive process with order 1 rather than using a gaussian process-based method as we have done in other work [@EpiNow2].
This represents a parsimonious approach in that we encode our expectation that the growth rate should vary over time and allow this to influence the forecast but we include only a single lag term, reducing the computational overhead of the model.
To model potential unobserved interventions and more general changes in transmission, we include an additional growth rate modifier restricted to be between 0 and 1 that differs depending on if the growth rate is positive or negative (due to potential differing responses when cases are growing or increasing) and that acts in a multiplicative fashion (meaning that larger absolute growth rates are reduced to zero growth more rapidly).
This reflects a simplified interpretation of how the ensemble appears to react to potential future changes by assuming a gradual return to stable incidence. 
The only observation for which we do not make an adaption is the apparent sharpness of the ensemble compared to our prior forecasting models choosing instead to make use of a negative binomial observation model allowing the inclusion of overdispersion. 
This choice is motivated by our belief that the underlying transmission process is an exponential one and therefore a count error model (where variance is linked to the mean)  is a sensible choice though this is an area of current research.
We suggest that part of the reason the hub ensembles exhibit this behaviour is due to the penalisation of overprediction compared to underprediction caused by the use of a generalised form of absolute error for the majority of forecast evaluations. 

In this study, we define an initial attempt at a surrogate model to replicate the observed behaviour of current multi-team forecast ensembles whilst maintaining a set of clear assumptions and summarising its implementation, with a focus on minimal resource use and reproducibility, as a GitHub action. 
We briefly outline some of the summary statistics it produces and their epidemiological relevance. 
We then evaluate its real-time performance in comparison to the ECDC forecast hub ensemble by visualising forecasts, using the weighted interval score [@REF], a commonly used proper scoring rule, and quantifying its empirical coverage.
We attempt to highlight settings where this model performs well as a surrogate to the ensemble forecast and areas where more work is needed. 
Finally, we summarise our findings, discuss their implications, and highlight areas for potential study. We aim for this work to highlight some of the potential implicit assumptions of current forecast hub ensembles, provide a sensible, low-resource, surrogate model in settings where large-scale collaborative forecasting efforts are not possible, and provide inspiration for forecasters looking to make principled improvements to their models

# Materials and Methods {-}

## Data {-}

```{r packages, cache = FALSE}
library(data.table)
library(purrr)
library(scoringutils)
library(lubridate)
library(forcats)
library(ggplot2)
library(ggridges)
library(patchwork)
library(scales)
library(here)
```

```{r load-functions}
source(here("R", "utils.R"))
source(here("R", "evaluate.R"))
source(here("R", "evaluation-plots.R"))
source(here("R", "coverage-plots.R"))
```

```{r data}
# Load JHU frozen truth data. See data-raw/get-truth.R for munging
truth <- fread(here("data", "truth.csv"))

# Load population data
population <- fread(here("data", "population.csv"))

# Load forecast metedata. See data-raw/get-hub-metadata.R for munging
metadata <- fread(here("data", "metadata.csv"))

# Load ECDC forecasts. See data-raw/get-hub-forecasts.R for munging
# Merge with truth and rescale to incidence rate per 100,000
forecasts <- fread(here("data", "forecasts.csv")) |>
  merge_forecasts_with_truth(truth) |>
  rescale_to_incidence_rate(population, scale = 1e4) |>
  rename_models()

# Load forecast scores
scores <- fread(here("data", "scores.csv")) |>
  rename_models()
```
  
  We extracted forecasts and data on notified weekly COVID-19 cases from the ECDC forecasting hub from the 15th of January 2022 to the 19th of July 2022 for the ensemble model (referred to as the `EuroCOVIDhub-ensemble` by the hub team) and the surrogate model (submitted as `epiforecasts-weeklygrowth`).
  We included all locations covered by the ECDC forecasting hub which were 32 European countries, including all countries of the European Union and European Free Trade Area, and the United Kingdom.
  Data on notified weekly cases was originally sourced from the Johns Hopkins University (JHU) curated data repository [@REF].
  We used the latest available observed data as of the 20th of August 2022. As observations are subject to revisions this means that the data used to produce forecasts for a given date may not reflect the data used for evaluation.
  This follows the common practice of the forecasting hub projects in treating the latest available data as the forecast target regardless of the number of potential data revisions.
  Incidence was aggregated over the Morbidity and Mortality Weekly Report (MMWR) epidemiological week definition of Sunday through Saturday.
  
  The ECDC forecast hub requests forecasts for one to four-week forecast horizon and requires forecasts to use a pre-specified format with 23 quantiles of the predictive probability distribution.
  No restrictions were placed on who could submit forecasts and the hub team actively invited participation from research groups known to be involved with COVID-19 forecasting projects.
  Forecasters used a wide range of forecasting methods including statistical approaches, mechanistic models including deterministic ordinary differential equation models and agent-based models, semi-mechanistic models, expert elicitation models, and ensembles of multiple approaches [@https://covid19forecasthub.eu/community.html].
  Teams submitted forecasts at the latest two days after the complete dataset for the forecast week became available and were allowed to use all data available at the time of submission (i.e including up to two days of data for the current week). 
  The ensemble forecast was constructed by taking the median of all predictive quantiles without the exclusion of any validly submitted forecast (where validity was defined as passing minimal formatting checks by the hub team). 
An ensemble was only produced for locations with at least 3 independent forecast models including the hub baseline model.
  Submitted forecasts and target observations are available from the ECDC forecast hub GitHub repository [@REF]. We provide code in the repository of this study to streamline access. 

```{r metadata}
# Number of locations
locations <- metadata |>
  DT(, uniqueN(location))

# Number of models overall
models <- metadata |>
  DT(, uniqueN(model))

# Number of forecast dates
forecast_dates <- metadata |>
  DT(, uniqueN(target_end_date))

# Models per forecast date
models_per_forecast <- metadata |>
  summarise_forecasts_by(by = "target_end_date")

# Models per location
models_per_location <- metadata |>
  summarise_forecasts_by(by = "location")

# Models with a location target
locations_per_model <- metadata |>
    summarise_forecasts_by(var = "location", by = "model")

locs_per_model_single_date <- metadata |>
    summarise_forecasts_by(
      var = "location", by = c("model", "target_end_date")
    ) |>
    DT(, unique(.SD[n == max(n), .(n)]), by = "model")

locs_per_model_date <- metadata |>
  summarise_forecasts_by(
    var = "location", by = c("model", "target_end_date")
  ) |>
  DT(locs_per_model_single_date[, .(model, max_n = n)], on = "model") |>
  DT(, n_per := n / max_n) |>
  DT(, .SD[!all(n == max_n)], by = "model")

models_per_loc_date <- metadata |>
  summarise_forecasts_by(var = "model", by = c("target_end_date", "location"))

# Single target models per location
single_locs_models_per_loc <- metadata |>
  DT(locations_per_model[n == 1], on = "model") |>
  summarise_forecasts_by(by = "location")

# Local models
local_models <- locations_per_model |>
  DT(n == 1) |>
  DT(, model)

# Global models
global_models <- locations_per_model |>
  DT(n > 30) |>
  DT(, model)

# Partial coverage models
partial_models <- locations_per_model |>
  DT(n > 2) |>
  DT(n <= 30)

# Varying submissions
varying_submissions <- locs_per_model_date |>
  DT(partial_models[, .(model)], on = "model")

# Models coverage of forecast dates
forecast_dates_by_models <- metadata |>
  summarise_forecasts_by(var = "target_end_date", by = "model") |>
  DT(order(n)) |>
  DT(, n_per := n / forecast_dates) |>
  DT(, type := fcase(
      model %in% global_models, "global",
      model %in% local_models, "local",
      model %in% partial_models[, model], "partial",
      default = "other"
    )
  )
```

## Model {-}

### Definition {-}

We model the expectation ($\lambda_t$) of reported cases ($C_t$) as an order 1 autoregressive (AR(P)) process by epidemiological week ($t$).
The model is initialised by assuming that the initially reported cases are representative with a small amount of error (2.5%).
We assume a negative binomial observation model with overdispersion $\phi$ for reported cases ($C_t$).

\begin{align*}
  \lambda_0 &\sim \text{LogNormal}\left(\log C_0 , 0.025 \times \log C_0 \right)\\
  \lambda_t &= C_{t-1} e^{r_t},\ t > 0  \\
  C_{t} \mid \lambda_{t} &\sim \text{NB}\left(\lambda_t, \phi\right)
\end{align*}

Where $r_t$ can be interpreted as the growth rate. $r_t$ is then modelled as a piecewise constant differenced AR(1) process modified such that the dependence of $r_{t-1}$ is multiplied by a decay factor ($\xi_{+,-}$) that varies dynamically according to the sign of $r_{t-1}$.
This assumes that the growth rate is non-stationary with a trend that is independent of the current growth rate (the differenced AR(1) process), the additional decay factor encodes the belief that larger absolute growth rates will tend more quickly towards no growth and that this process may work differently for positive or negative growth rates.
This process can be defined as follows, 

\begin{align*}
  r_0 &\sim \text{Normal}\left(0, 0.25 \right) \\
  r_t &= \left(\mathcal{H}(r_{t-1} > 0) \xi_{+} + \mathcal{H}(r_{t-1} \le 0) \xi_{-}\right)r_{t-1} + \epsilon_t  \\
  \epsilon_t &= \mathcal{H}(t > 0) \beta \epsilon_{t-1} + \eta_t
\end{align*}

Where $\mathcal{H}$ is the Heaviside step function and is defined such that it attains the value of 1 if the argument is true and the value of 0 otherwise.
The following priors are used,

\begin{align*}
  \xi_{+} &\sim \text{Beta}\left(3, 1 \right) \\
  \xi_{-} &\sim \text{Beta}\left(3, 1 \right) \\
  \beta &\sim \text{Normal}\left(0, 0.25 \right) \\
  \eta_t &\sim \text{Half-Normal}\left(0, \sigma \right) \\
  \sigma &\sim \text{Normal}\left(0, 0.2 \right) \\
  \frac{1}{\sqrt{\phi}} &\sim \text{Half-Normal}(0, 1) 
\end{align*}

Where $\sigma$, and $\frac{1}{\sqrt{\phi}}$ are truncated to be greater than 0 and $\beta$ is truncated to be between -1 and 1.
The Beta priors for $\xi_{+,-}$ have been chosen to be weakly informative that the reduction towards 0 growth is relatively slow.
Similarly the prior for $\beta$ has been chosen to be weakly informative that there is weak auto-correlation in differenced growth rates.
$\sigma$ has also been made weakly informative under the assumption that the potential change in growth rates in a single timestep should be relatively small.

### Summary statistics {-}

As well as posterior predictions and forecasts for notifications the model also returns several epidemiological summary statistics which may be useful for drawing inferences about underlying transmission dynamics.
These are the log scale weekly growth rate ($g^{o, \delta}_t$), and the instantaneous effective reproduction number ($R^{o, \delta}_t$).
These are calculated as follows:

\begin{align*}
  g^{o, \delta}_t &= T_s r^{o, \delta}_t \\
  R^{o, \delta}_t &= e^{\left(T_s r^{o, \delta}_t\right)} \\
\end{align*}

$T_s = 7$ is a parameter that defines the weekly timespan over which the summary metrics apply dependent on the time step of the data.

## Forecast evaluation {-}

To standardise forecasts across forecast locations we first normalised both weekly notified test positive cases and forecast test positive cases by the population in the forecast region scaled to be an incidence rate per 10,000 people.
We then evaluated a subset of forecasts from the following countries Germany, Greece, Italy, Poland, Slovakia, and the United Kingdom visually by forecast horizon (1-4 weeks) for both the ensemble and surrogate model. 
These countries were selected to include forecasts based on different numbers and types of submitted forecast models, to be at least partially representative of the full sample of forecast locations, and to include nations for which the authors had a good understanding of local data and transmission dynamics in the study period. 

We evaluate forecasts for all locations and horizons quantitatively using the weighted interval score (WIS) [@bracherEvaluatingEpidemicForecasts2021], which is a quantile-based proper scoring rule that approximates the continuous ranked probability score (CRPS).
Both the WIS and CRPS are generalisations of the absolute error to evaluate probabilistic forecasts and are widely used to evaluate COVID-19 forecasts, including by the ECDC forecast hub  [@REF].
We present WIS for the subset of forecasts we explore visually for both the ensemble and surrogate model by date and forecast horizon (1 and 4 weeks).

To understand the relative performance of the surrogate model compared to the ensemble model, we calculate the relative performance (rWIS) by dividing the WIS for the surrogate model by the WIS of the ensemble model for all locations and forecast horizons.
To maintain the propriety of this score, we do this after first taking the means of scores for the relevant stratification.
We explore relative performance by forecast horizon, by month and horizon, and by location and horizon. 

In addition to presenting the WIS for a subset of locations and the relative WIS for all locations, we also calculate and visualise the empirical coverage, which is the percentage of observed values within a given interval or below a given quantile, of both the surrogate and ensemble model for the 30%, 60%, and 90% credible intervals and by quantile. 
Lastly, we calculate and visualise the relative weighted interval score by quantile, stratified by forecast horizon.

```{r eval-parameters}
locs <- c("United Kingdom", "Germany", "Slovakia", "Italy", "Poland", "Greece")
ranges <- c(30, 60, 90)
```

```{r coverage}
coverage <- calc_coverage(scores)
```

```{r relative-interval-score}
relative_interval_score <- scores |>
  calc_relative_score(
    cols = c("location", "target_end_date", "horizon", "range")
  )

overall_ae_median <- scores |>
  DT(, as.list(summary(ae_median)), by = "model")

wis <- scores |>
  summarise_scores(
    by = c("location_name", "target_end_date", "horizon", "model")
  )
relative_wis <- wis |>
  calc_relative_score(cols = c("location_name", "target_end_date", "horizon"))

relative_wis_by_horizon <- scores |>
  summarise_scores(by = c("horizon", "model")) |>
  calc_relative_score(cols = c("horizon")) |>
  DT(, ris := round(relative_interval_score, 2))

relative_median_wis_by_horizon <- relative_wis |>
  DT(,
     .(relative_interval_score = median(relative_interval_score)),
     by = horizon
  ) |>
  DT(, ris := round(relative_interval_score, 2))

relative_wis_thresholds <- relative_wis |>
  DT(,
    .(
      better_than = sum(relative_interval_score < 1) / .N,
      fifty_percent = sum(relative_interval_score < 1.5) / .N,
      more_than_100_percent = sum(relative_interval_score > 2) / .N
    )
  ) |>
  DT(, map(.SD, ~ round(. * 100, digits = 0))) |>
  DT(, map(.SD, ~ paste0(., "%")))
```

## Implementation {-}

The model is implemented in `stan` [@stan] and `R` (`4.2.0`) [@R] as an extension of the baseline model from the `forecast.vocs` R package (`0.0.9.7000`)  [@forecast.vocs].
We note that our use of a Heaviside step function introduces a discontinuity to the posterior making it less suited for use with `stan`. Other model formulations without this feature would be more efficient and robust.
The `cmdstanr` R package (`0.5.2`) [@cmdstanr] is used for model fitting with 2 MCMC chains each having 1000 warm-up and 1000 sampling steps each [@cmdstanr].
`cmstanr` surfaces several settings that trade off between sampling speed and the robustness of the approach.
Here we take a conservative approach, as the model fit is not manually inspected during real-time usage and due to the expected complexity of the posterior [@betancourt_2017], and set the adapt delta setting to 0.99, and the maximum tree depth setting to 15. 
For real-time usage convergence was not assessed but during model development, the Rhat diagnostic was used alongside feedback from `cmdstanr` about the number of divergent transitions and exceedance of the maximum tree depth [@cmdstanr]. During development, posterior predictions were also visually compared to observed data.

To download and manipulate forecasts from the ECDC forecasting hub [@EuroHub] we use  the `data.table` (`1.14.2`) [@data.table] and `gh` (`1.3.0`) [@gh] R packages.
We make use of further functionality from the `forecast.vocs` R package [@forecast.vocs] to prepare data for forecasting, visualise forecasts and summary measures, and summarise forecasts.
Forecast evaluation is implemented using the `scoringutils` R package (1.0.0) [@scoringutils], and the `scoringRules` R package (1.0.1) [@REF].

To ensure the reproducibility of this analysis dependencies are managed using the `renv` R package (`0.14.0`) [@renv] and a Dockerfile file along with a built Docker image [@Boettiger:2015dw] (via GitHub Actions) is provided in the code repository.
Weekly forecasts are also made using `renv` and GitHub actions to ensure they require limited compute and that our implementation is independent of local resources facilitating democratised access.
The code for this analysis can be found here: https://github.com/epiforecasts/simplified-forecaster-evaluation
The code for the forecasting model defined above along with the infrastructure required to forecast using GitHub Actions can be found here: https://github.com/seabbs/ecdc-weekly-growth-forecasts
Versions archived on Zenodo are available [@REF] and [@REF].

# Results {-}

## Data summary {-}

In our study period, incidence rates across ECDC nations and in the UK were primarily driven by the spread of novel subvariants of concern related to the Omicron variant and changes in population susceptibility. Many nations, such as the UK saw large BA.1 waves in January that then resulted in declining incidence rates through February.
From late February, through to the end of May, most nations then saw another wave, typically with lower reported incidence rates, driven by BA.2. This wave was typically characterised by a lower peak than the BA.1 wave with a more gradual decrease in incidence.
The end of our study period was characterised by the gradual take-over of the BA.4/BA.5 subvariants that again had a lower peak and lower absolute growth rates. Unlike earlier periods in the pandemic, our study period did not see the use of new non-pharmaceutical interventions (NPIs) in response to increasing COVID-19 incidence in most locations.
In addition, in general, ascertainment rates likely reduced over time in most locations due to reductions in routine testing, and reductions in test availability. Whilst both the reduced use of NPIs and testing generally occurred across nations our study period also marked an increase in the heterogeneity of the response to the COVID-19 pandemic with nations changing policy at different times and to different degrees.
This is in contrast to the early COVID-19 pandemic response for which most nations took similar actions at similar times.

We extracted forecasts starting from the 15th of January until the 19th of July 2022 for all countries covered by the ECDC forecasting hub (all ECDC nations and the UK making `r locations` unique locations).
In total `r nrow(metadata)` forecasts were made across all locations, `r forecast_dates` forecast dates from `r models` independent forecast models (including the ECDC baseline model).
Of these models, `r length(global_models)` forecasted in at least 30 locations including our original submission  (referred to as `epiforecasts-EpiNow2` by the hub), and our surrogate model.
Of the remaining models submitted `r length(local_models)` were submitted in only one location.
Single location models were clustered in a few locations, particularly Germany and Poland (likely due to the folding of the German/Poland forecasting hub into the ECDC forecasting hub project).
Italy was also an outlier with `r single_locs_models_per_loc[location == "IT", n]` models that submitted nowhere else.
`r nrow(partial_models)` models were submitted for between 3 and 30 locations and all these models varied the number of locations they submitted forecasts for over time, potentially indicating manual curation or models targeted at specific conditions.

Across all forecast dates and locations the minimum number of independent forecasts was `r models_per_loc_date[, min(n)]` with the maximum being `r models_per_loc_date[, max(n)]`. The median number of independent forecasts per location and forecast date was `r models_per_loc_date[, median(n)]`. All locations received forecasts from at least `r models_per_location[, min(n)]` models with the median number of forecast models per location being `r models_per_location[, median(n)]`. Coverage of forecast dates varied across submitted models with `r forecast_dates_by_models[n_per == 1, .N]` models submitting for all dates, `r forecast_dates_by_models[n_per >= 0.9, .N]` models submitting for at least 90% of dates, and `r forecast_dates_by_models[n_per < 0.50, .N]` models submitting for fewer than 50% of forecast dates. In general, there was no clear difference in forecast date coverage between models that submitted for all locations vs a small subset but models with partial coverage of locations all also had partial coverage of forecast dates.

## Forecast evaluation {-}

### Visualisation of forecasts by horizon {-}

In our example set of locations, the absolute performance of the ensemble and the surrogate model was similar in all locations at short forecast horizons though this varied by location.
Performance was most comparable in Slovakia but in the United Kingdom and Germany in particular the surrogate model performed visibly worse for some forecast dates (Figure \@ref(fig:vis-forecasts)).
For both the ensemble and the surrogate performance visibly decreased as the forecast horizon increased though the relative difference in performance between models was relatively stable across horizons.
In general, in the study period, the ensemble appeared to be liable to underpredict and the surrogate model appeared more likely to over-predict. 
Both models forecast large reductions in incidence in Poland during May that did not occur whilst only the ensemble forecast spuriously forecast similar large reductions in Germany during June.
In comparison to the ensemble model the surrogate model appeared less likely to place weight on infeasibly large reductions in incidence during periods of declining incidence but on other hand was more likely to forecast continuing increases in incidence (for example in February in Slovakia and Poland).
Italy and Germany, which had the highest number of submitted models of the example locations had little clear difference in performance between the ensemble and surrogate models compared to other example locations.

```{r vis-forecasts, fig.cap = "a.) Weighted interval scores at the one-week and four-week forecast horizon by epidemiological week in Germany, Greece, Italy, Poland, Slovakia, and the United Kingdom. b.)Forecasts of notified test positive cases (per 10,000 population) by epidemiological week in Germany, Greece, Italy, Poland, Slovakia, and the United Kingdom,  by forecast horizon (one to four weeks). Incidence rates are shown on a log scale. 30\\%, 60\\%, and 90\\% credible intervals are shown. The black line and points are the notified cases as of the date of data extraction rather than those available at the time.", out.width = "95%", fig.height = 12, fig.width = 18}
p <- (
  plot_wis(wis, locs) +
   theme(strip.text.x = element_blank()) |
  plot_forecasts(forecasts, locs, ranges) +
    guides(fill = guide_none(), fill_ramp = guide_none(), col = guide_none())
) +
  plot_layout(widths = c(1, 4), guides = "collect") +
  plot_annotation(tag_levels = "a") &
  theme(
    legend.position = "bottom", legend.margin = margin(),
    legend.justification = "centre"
  )
suppressWarnings(print(p))
```

### Relative forecast evaluation {-}

Evaluating the ensemble and surrogate models using the weighted interval score across all locations and forecast dates we found that the mean relative performance of the surrogate model was `r relative_wis_by_horizon[horizon == 1, ris]` at the one week horizon, `r relative_wis_by_horizon[horizon == 2, ris]` at the two week horizon, `r relative_wis_by_horizon[horizon == 3, ris]` at the three week horizon, and `r relative_wis_by_horizon[horizon == 4, ris]` at the four week horizon.
Indicating that the ensemble forecast outperformed the surrogate forecast for all horizons by approximately 100% on average with the surrogate model performing worst at the one and four-week horizon.
However, much of this outperformance was driven by a small subset of forecasts with relative performance having a heavy tail (Figure \@ref(fig:assemble-eval) a). 
If we instead consider median relative performance (note not a proper score) we find that, relative to the ensemble, the surrogate scored `r relative_median_wis_by_horizon[horizon == 1, ris]` at the one week horizon, `r relative_median_wis_by_horizon[horizon == 2, ris]` at the two week horizon, `r relative_median_wis_by_horizon[horizon == 3, ris]` at the three week horizon, and `r relative_median_wis_by_horizon[horizon == 4, ris]` at the four week horizon.
Looking across the distribution of relative scores by horizon we see little evidence that relative forecast quality decreased with time, though the tail of the distribution appears slightly heavier at the four-week time horizon (Figure \@ref(fig:assemble-eval) a).
`r relative_wis_thresholds[, better_than]` of surrogate forecasts scored better than the comparable ensemble forecast, `r relative_wis_thresholds[, fifty_percent]` performed within 50% of the comparable ensemble forecat, and scored `r relative_wis_thresholds[, more_than_100_percent]` performed worse than 100% more than the comparable ensemble forecast.

The surrogate model's relative performance varied over time with substantially worse performance from January to March compared to later in the year across all forecast horizons (Figure \@ref(fig:assemble-eval) b).
The majority of the difference in performance appeared to be driven by a thicker right tail with this being a particular feature of forecasts at longer horizons. Notably, forecast performance in March had a bimodal distribution at the four-week horizon with a substantial fraction of surrogate forecasts outperforming the ensemble.
This variation in performance may have been linked to the BA.2 wave which peaked in most locations during this period.

There was also substantial variation across forecast locations with the surrogate performing well in some locations at some forecast horizons, for example, the four-week horizon in the United Kingdom, and badly in others, for example, the four-week forecast in Switzerland (Figure \@ref(fig:assemble-eval) c).
In general, across locations, as observed overall, relative forecast performance was comparable across horizons though with a heavier right tail at longer horizons. Some locations showed less of this behaviour, for example, Spain, and in some, it was very dominant, for example, Switzerland. 

```{r make-rwis-plots}
plot_rwis_horizon <- relative_wis |>
  copy() |>
  DT(, horizon := fct_rev(as.factor(horizon))) |>
  plot_relative_wis(
    y = horizon, fill = horizon,
  ) +
  scale_fill_brewer(palette = "Dark2") +
  scale_color_brewer(palette = "Dark2") +
  labs(y = "Forecast horizon (weeks)") +
  guides(fill = guide_none(), point_color = guide_none(), point = guide_none())

plot_rwis_location <- relative_wis |>
  DT(horizon == 1 | horizon == 4) |>
  plot_relative_wis(
    y = location_name, fill = as.factor(horizon), alpha = 0.3,
    jittered_points = FALSE, quantiles = NULL,  point_color = horizon
  ) +
  scale_fill_brewer(palette = "Dark2") +
  scale_color_brewer(palette = "Dark2") +
  labs(
    y = "Forecast location", fill = "Forecast horizon (weeks)",
    point_color = "Forecast horizon (weeks)"
  )

plot_rwis_by_month <- relative_wis |>
  DT(horizon == 1 | horizon == 4) |>
  DT(, target_month := month(target_end_date, label = TRUE)) |>
  DT(, target_month := fct_rev(target_month)) |>
  plot_relative_wis(
    y = target_month, fill = as.factor(horizon), alpha = 0.3,
    jittered_points = TRUE, quantiles = NULL
  ) +
  scale_fill_brewer(palette = "Dark2") +
  labs(y = "Target forecast month", fill = "Forecast horizon (weeks)") +
  guides(fill = guide_none())
```

```{r assemble-eval, fig.height = 12, fig.width = 12, out.width = "95%", fig.cap = "Relative weighted interval score by location, horizon, and forecast date for the surrogate forecast model compared to the ensemble forecast model on the log scale. a.) The density of the relative score by horizon. Horizontal black lines give the 5\\%, 35\\%, 65\\%, and 95\\% quantiles. b.) The density of the relative score by month for a given forecast horizon stratified by the one and four-week forecast horizon. c.) The density of the relative score by forecast location stratified by the one and four-week forecast horizon. The dashed line on all plots indicates when the ensemble forecast is equivalent to the surrogate forecast. The vertical black lines on the y-axis give individual relative scores."}
p <- (
  (
    (
      plot_rwis_horizon / plot_rwis_by_month
    )  +
    plot_layout(heights = c(4, 5), guides = "collect")
  ) |
  plot_rwis_location
) +
  plot_layout(guides = "collect", widths = c(1, 1)) +
  plot_annotation(tag_levels = "a") &
  theme(
    legend.position = "bottom", legend.margin = margin(),
    legend.justification = "centre"
  )
suppressWarnings(print(p))
```

### Forecast calibration {-}

Overall the surrogate model was well calibrated at the 30%, 60% and 90% credible intervals whilst the ensemble model was in general poorly calibrated with calibration becoming worse at larger credible intervals.
When stratified by forecast horizon the ensemble forecast was best calibrated at the one-week forecast horizon and then became progressively less well calibrated as the forecast horizon increased with the worst calibration at the 90% credible interval (Figure \@ref(fig:plot-coverage) a). 
In comparison, the surrogate forecast was less well calibrated at the one-week forecast horizon with a tendency to have a larger empirical coverage than required at longer horizons the surrogate forecast transitioned to being well calibrated and then had lower coverage than optimal though to a much lower degree than the ensemble forecast.
This was not the case for the 90% credible interval where the surrogate model stably covered slightly more than the expected interval indicating forecasts were somewhat overly uncertain for this interval. 

Stratifying calibration by quantile and forecast horizon the ensemble forecast was overly precise at all horizons for quantiles larger than the median whilst being comparably well calibrated for intervals below the median (Figure \@ref(fig:plot-coverage) b).
In contrast, the surrogate ensemble forecast was generally equally well calibrated at shorter horizons with a tendency to produce conservative forecasts.
At longer horizons, however, quantiles below the median were liable to have poor coverage suggesting a bias towards over-predicting.

Breaking down the relative weighted interval score by forecast interval we observe that the majority of the difference in overall scores is likely driven by larger intervals and in particular the tails of the forecast (Figure \@ref(fig:plot-coverage) c).
This is true across forecast horizons but the magnitude of the different increases. This implies that the surrogate and ensemble model produced comparably performing point forecasts and indeed if we consider the absolute error of the median forecasts we see that the median for the ensemble forecast was `r overall_ae_median[model == "Ensemble", round(Median, digits = 1)]` whilst the median for the surrogate forecast was `r overall_ae_median[model == "Surrogate", round(Median, digits = 1)]`.
However, when we consider the mean absolute error of the median we find that the ensemble forecast outperformed the surrogate forecast (`r overall_ae_median[model == "Ensemble", round(Mean, digits = 1)]` vs. `r overall_ae_median[model == "Surrogate", round(Mean, digits = 1)]`) as noted elsewhere.
This indicates a right skew in performance for the surrogate model point forecasts not observed for the ensemble forecast which matches that observed when evaluating the full predictive distributions of both forecasts.

```{r plot-coverage, fig.height = 12, fig.width = 18, out.width = "95%", fig.cap = "a.) Empirical coverage of the surrogate and ensemble forecasts at the 90\\%, 60\\%, and 30\\% credible intervals stratified by forecast horizon. Ideally, a well-calibrated forecast should have empirical coverage for a given credible interval that equals the range of that interval (i.e 90\\%). b.) Median relative weighted interval score by quantile and forecast horizon for the surrogate forecast compared to the ensemble forecast. c.) Empirical coverage by quantile for both the surrogate and ensemble forecasts. A well-calibrated forecast should have empirical quantiles that match the theoretical ones. The green area of this figure corresponds to conservative forecasts."}
p <- (
  (
    (
      plot_coverage_range(coverage, ranges) /
      plot_rel_score_by_quantile(relative_interval_score)
    )  +
    plot_layout(heights = c(3, 1), guides = "collect")
  ) |
  plot_coverage_quantiles(scores)
) +
  plot_layout(widths = c(1, 2), guides = "collect") +
  plot_annotation(tag_levels = "a") &
  theme(
    legend.position = "bottom", legend.margin = margin(),
    legend.justification = "centre"
  )
suppressWarnings(print(p))
```

# Discussion {-}

## Summary {-}

In this study, we defined a surrogate model aiming to replicate some of the observed behaviour of the ECDC forecast hub multi-team ensemble for forecasting test-positive reported COVID-19 cases in ECDC nations and the UK.
We first defined a set of assumptions for how the surrogate model should behave based on our observations of the ECDC forecast hub ensemble, our experience submitting forecasts to various forecast Hubs, and our observations of the performance of other models.
We restricted ourselves to a model that could be easily understood, that produced epidemiologically meaningful summary statistics, and that could be run with low compute resources.
We further provide a fully reproducible workflow for running and evaluating this model using GitHub actions facilitating others to do the same. 

Over the 6 months of the study period, we found that our surrogate model produced forecasts that were visually similar to those from the forecast hub ensemble though with greater uncertainty.
In a subset of example locations, we observed some variation in performance across locations, that the ensemble appeared biased toward underprediction, and the surrogate model appeared biased towards overprediction.
Evaluating the relative performance of the surrogate model compared to the ECDC forecast hub ensemble we found that the mean performance was substantially worse but that this was largely driven by a relatively small subset of forecasts.
The median relative performance was only marginally worse across forecast horizons with the distribution of relative scores having a heavy right tail.
Performance varied by location and forecast date with the surrogate model performing worse in the first part of 2021 which may have been linked to increasing incidence rates across forecast locations linked to BA.2.
Unlike our earlier forecasting efforts, the effect of outlier forecasts was much reduced though the surrogate model was less likely to outperform the ensemble than our earlier forecasting model methodology [@refNikosinGermany].
In general, the relative performance of the surrogate model was stable across forecast horizons, unlike our previous methods which rapidly became less performant, though the distribution of relative performance did have an increasingly heavy right tail as the forecast horizon increased indicating a greater share of forecasts performing very poorly in comparison the hub ensemble.
As noted elsewhere, the forecast hub ensemble was poorly calibrated, particularly at longer forecast horizons and bigger credible intervals whereas the surrogate model was substantially better calibrated for all targets though it tended to overcover at large intervals.
Stratifying the relative weighted interval score by interval we found that larger intervals (i.e the tails of the forecast distribution) were responsible for the majority of the difference in performance. 
Indeed, the median point performance of the forecasts was comparable though the surrogate model performed substantially worse on average due to its skewed performance distribution.

## Limitations {-}

Our study benefits from having been conducted using forecasts conducted in real-time, rather than retrospectively, and submitted to an independent forecast research hub (though we note the overlap between authors on this study and the ECDC forecast hub).
This means that we can be confident we have captured how our forecast methodology behaves in real-world usage and that others can evaluate this for themselves. 
The downside of this approach is that it was not possible to update the surrogate model over time in response to initial evaluation or to explore other parameterisations that might be more successful of which there are likely several.
However, as our study has been conducted with a focus on reproducibility and openness our findings can be replicated or extended by others regardless of compute availability (due to our use of GitHub actions as a compute platform which is freely available to researchers).
Importantly in this study, we focussed on replicating the forecast hub ensembles' observed behaviour rather than attempting to define an optimal forecast for forecast consumers.
It is likely that if we had instead aimed to develop a forecast methodology that minimised the evaluation criteria we planned to use, especially if we relaxed our assumed compute resource constraints, we would have produced forecasts that performed better relative to the hub ensemble.
However, if we start from the view that the forecast hub ensemble has traits that are desirable for use by policy-makers (i.e robustness and good average performance) then our approach makes sense and may be optimal for producing a "good" forecast.
This is the view often used in the literature when describing forecast ensembles.
Our focus on replicating the performance of the hub ensemble is also useful as it highlights some of the emergent behaviour of the ensemble captured in our assumptions, such as auto-correlation across time points, modelled decrease in growth rate as the forecast horizon increases
It also highlights some of the differences between our surrogate forecast model and the ensemble that may lead to new insights, such as the generally poor coverage of the ensemble that could not be explained by the assumptions we used in developing our surrogate methodology.
Whilst we normalised reported cases to be population-adjusted incidence rates our results are conditional on the use of the weighted interval score as an evaluation metric.
As this proper scoring rule measures absolute performance this means that forecasts during periods of higher incidence are given more weight than forecasts from periods of low incidence.
It also means that potentially overpredicting is penalised more than underpredicting as incidence rates are bounded at zero but relatively weakly bounded by populations at the upper bound (as incidence rates are typically only a small fraction of the overall population).
This bias could explain the relatively poor performance of the surrogate model, compared to the ensemble, despite the surrogate model being comparably well-calibrated.
We consider alternative methods of forecast evaluation that would be robust to this potential source of bias but choose to stick closely to the methodology used by the ECDC forecast hub as these choices inform the development of submitted models and so are key to our findings.

## Literature context {-}

There are no other studies in the epidemiology literature which we are aware of that attempt to develop a forecasting model based on the observed behaviour of a multi-team, multi-model ensemble.
Few studies focus on delivering computationally feasible forecasting models in a reproducible framework backed by an openly accessible compute platform. 
However, the CDC, ECDC, and Germany/Poland forecasting hubs have published a range of evaluations of forecasts submitted to their platforms and the relative performance of their ensembles. 
In general, these studies have struggled to draw general conclusions about the structural assumptions of forecast models they consider "good" (generally they have defined this as minimising the weighted interval score as done in this study).
The CDC forecast hub identified the top 5 performing models and noted they made the following structural assumptions.
However, they did not extensively compare and contrast these conclusions to arrive at a set of desired forecast assumptions as done in this study to motivate the surrogate model.
Similarly, the Germany and Poland, and ECDC forecasting hubs were able to identify forecast models that performed comparably well but did not derive structural assumptions that led to this performance or detail explicitly what the desirable performance characteristics would be, aside from optimising the weighted interval score.
All comparable forecast hub projects found that their ensemble was often the best choice, had desirable characteristics such as robustness, though this was rarely fully defined, and should be the output used by forecast consumers.
In general, during the study period, all projects used the same median ensemble forecast of all submissions after having concluded it was a favourable choice. 
The CDC forecasting hub also evaluated a range of other ensemble approaches, such as inverse weighted interval score weighting, unweighted ensembles of a selection of models based on recent performance, and mean ensemble.
Work on this is still ongoing but these more complex ensembling approaches were shown to outperform the median of all submitted forecasts in many cases.
That being said no forecast hub has switched to these for their operational forecast implying they do not think the evidence base is strong enough for them to be used by forecast consumers and hence the median of all submitted forecasts remains the community-suggested default ensemble option. 
Other studies have been published evaluating single forecast models in comparison to ensemble performance from the forecast hub but in general, these have not focussed on replicating ensemble behaviour but rather optimising the target evaluation metric. 
Our previous work also highlighted the lack of calibration in an ensemble forecast from the Germany/Poland forecasting hub compared to forecasts from epidemiological models and also noted the bias towards underprediction observed in the ensemble forecasts and not the model-based forecasts [@refNikosinGermany].

## Further work {-}

Whilst we derived our surrogate model from a range of assumptions based on observing ensemble forecasts behaviour and the behaviour and structure of submitted models this was by no means as structured or complete a process as it could have been.
In follow-up work, a more rigorous approach to this could be taken to further refine this set of assumptions. 
The findings from our study may also be useful for informing these improved assumptions. 
In addition, the model we derived based on our assumptions was likely not optimal both in terms of compute time and accuracy at reproducing ensemble-like behaviour.
Models with a more complex auto-correlation structure and more refined approaches to localise trends should be explored to improve relative performance to ensemble forecasts.
An example of a family of possible approaches are structural time series model which has many of the characteristics implied by our assumptions for how forecast ensembles typically operate.
As we identified that the tails of our predictive distributions were responsible for a large proportion of the difference in performance compared to the forecast ensemble.
It may be the case that post-processing of forecasts from our surrogate model to enhance their similarity to the forecast ensemble is a sensible strategy.
This seems likely to improve out-of-sample performance but does not help with the secondary aim of understanding the implicit assumptions driving the performance of multi-model, multi-team infectious disease forecast ensembles.
As we have hypothesised that the use of absolute scoring measures is inappropriate and leads to performance characteristics that are unlikely to be favoured by forecast stakeholders more work should be done in this area.
If new forecast ensemble methods are adopted as best practice by forecast hubs then follow-up work attempting to create surrogate forecast models must also use these approaches and this will likely alter the observed characteristics of the hub ensemble forecasts, for example, the tendency to be poorly calibrated.
Lastly, here we have only explored a surrogate for an ensemble for a single disease and a limited set of locations.
Follow-up work should explore whether this behaviour holds across diseases and locations.
However, this work is limited by the lack of large-scale forecast ensembling projects despite them showing obvious promise to improve the forecasts available to stakeholders.

## Conclusions {-}

We conclude that our simplified forecast model may have captured some of the dynamics of the hub ensemble but that more work needs to be done to understand the epidemiological model that represents its behaviour and whether or not this is the optimal choice for stakeholders' requirements. We also conclude that our findings are largely driven by the choice of evaluation measure used by the forecast hub and that if this is misspecified then the forecast hub submissions may not be optimised to forecast users' expectations or requirements. Our work may be useful for forecast users to understand the inherent assumptions of the forecasts they are making use of and to methodology developers thinking about how to develop forecasts that perform similary to current multi-model and multi-team forecast ensembles that are trusted by stakeholders.

# Additional Information and Declarations {-}

## Competing interests {-}

JB and SF, and KS have received funding to run forecast hub projects.

## Author contribution {-}

SA conceived the study, developed the initial set of assumptions for the surrogate model, implemented the model into code, designed and conducted the forecast evaluation, and wrote the first draft of the manuscript. All other authors provided feedback on the manuscript and contributed to revisions. SF provided funding. HG reviewed the code and reproducibility of the analysis.

## Data availability {-}

All data and code are available here: 

https://github.com/epiforecasts/simplified-forecaster-evaluation

## Funding {-}

SA and SF were funded by a Wellcome senior fellowship to SF (), KS was funded by, and JB was funded by.

# Acknowledgments {-}

We thank the ECDC for supporting the forecasting hub, and all forecasters who submitted forecasts for making this study possible. We thank the forecast hub team for publishing all data in an accessible format. We thank Molly for being a good Labrador.

# References
